[{"토픽":"데이터 분석 처리 5단계","키워드":"정 수 전 모 시\n\n탐 전 모 평","도식":null,"정의":null,"내용":"문제 정의 > Data 수집 > 전처리 > 모델링 > 시각화\n4단계면 : 탐색(EDA) > 전처리 > 모델링 > 모델 평가"},{"토픽":"SEMMA","키워드":"샘 탐 수 모 평","도식":null,"정의":"Sample, Explore, Modify, Model, Assess 의 5단계데이터 분석 방법론(SAS 기업)","내용":"Sample > 탐색 > 수정 > 모델링 > 평가\n개발자 관점(비즈니스 관점 미흡)"},{"토픽":"CRISP-DM","키워드":"업 데 준 모 평 적","도식":null,"정의":"계층적 프로세스 모델로써,4개 레벨과 6단계로 수행되는데이터 분석 방법론(SPSS)","내용":"CRoss-Industry Standard Process for Data Mining\n레벨 : Phase(lv 구성), 일반화, 세분화, 구체적 마이닝 실행\n단계 : 업무 이해 > Data 이해 > Data 준비 > 모형 > 평가 > 적용"},{"토픽":"KDD\n(Knowledge Discovery in Database)","키워드":"선 전 변 마 평","도식":null,"정의":"데이터마이닝과 동의어,DW\/OLAP\/마이닝 툴 등 이용,대량의 Data 내 유용한 지식을발견하는 활동","내용":"인공지능의 한 분야, 미처 몰랐던 패턴 등 지식 발견 목적\n* 기계학습은 이미 아는 속성 통해 '예측, 자동개선' 목적\n각종 Data > DW, OLAP 서버(다차원 쿼리) > KDD 서버\nData 선택 > 전처리 > 변환 > 마이닝 > 평가"},{"토픽":"데이터 분석 성숙도 모델\n(가트너)","키워드":"설 진 예 처","도식":null,"정의":"원인 분석해 기대효과를 설정,설명~처방적 분석으로 갈수록난이도와 가치가 높아지는 모델","내용":"설명적 > 진단적 > 예측적 > 처방적 분석\n= 뭐가 발생? > 원인 무엇? > 어떻게 될까? > 뭘 할건가?\n= 기술 통계 > 원인 분석 > 패턴, 확률 사용 > To Do 제시"},{"토픽":"데이터 분석 거버넌스","키워드":"인 교 진 개 조","도식":null,"정의":"기업에서 의사결정을 위한 데이터를 어떻게 관리, 유지 및 규제 하는지에 대한 내부적인 관리 프로세스와 함께 데이터 분석의 지속적 개선\/개발, 확산 및 서비스관리를 위한 거버넌스 체계","내용":"분석 프로세스 - EDA(탐색적 데이터 분석) \/ CDA(확증적 데이터 분석)\n분석 전문 인력 - 데이터 사이언티스트\n분석 조직 - 데이터분석 컨트롤 타워\n분석 수준 진단 - 분석 준비도 \/ 분석 성숙도\n분석 교육 - 가설 검증 능력 확보"},{"토픽":"데이터 분석 성숙도","키워드":"비 조 아\n도 활 확 최","도식":null,"정의":null,"내용":"분야 : 비즈니스, 조직, IT 부문 3개를 각 도활확최로 평가\n도입 : 분석 환경, 시스템 구축(DW, DM, ETL, EAI, OLAP)\n활용 : 분석 결과 업무 적용(시뮬레이션, 미래 예측, 통계)\n확산 : 전사 차원 분석 공유(프로세스 혁신, 전사 성과 분석)\n최적화 : 혁신, 성과에 기여(전략 연계, 경영진 분석 활용)"},{"토픽":"데이터 분석준비도","키워드":"데 기 문 조 인 업","도식":null,"정의":null,"내용":"6영역 : Data, 분석기법, 문화, 조직, IT 인프라, 분석업무\n- Data : 정일유보적접근, MDM\n- 인프라 : EAI, ETL, DW, Data Lake, Fabric, Mesh\n- 업무 : 기술 통계, 예측, 시뮬레이션, 최적화, 지속 개선 등"},{"토픽":"데이터 분석 성숙도 \/ 준비도 매트릭스","키워드":"엑 준 와 성\n\n확 정 준 도","도식":null,"정의":null,"내용":"X축 : 준비도 \/ Y축 : 성숙도\n도입, 확산, 준비, 정착형(4 1 3 2)"},{"토픽":"머신러닝 파이프라인","키워드":"수 전 훈 배 예","도식":null,"정의":"데이터 수집부터 전처리, 학습 모델 배포, 예측까지 전과정을 순차적으로 처리하도록 설계된 머신러닝 아키텍처","내용":"필요성\n자동화 : 지속 수행 위한 파이프라인 기반 자동화\n예측 정확성 향상 : 내부 구조 이해를 통한 성능(예측정확성) 향상\n\n처리단계\n수집 > 전처리 > 모델 훈련 > 모델 배포 > 예측 제공\n수집 : DL, ETL\n전처리 : 정규화, 편향분석\n모델훈련 : 하이퍼파라미터 조정, 반복\n모델 배포 : 클라이언트, 백엔드, 역전파 훈련\n예측 제공 : 결과 생성"},{"토픽":"MLOps","키워드":"데 모 뎁\n개 I D T D 모\n\n디 트 운","도식":null,"정의":"DevOps 의 CI\/CD 요소에머신러닝 학습의 지속성 위한 CT까지 고려한 방법론","내용":"MLOps = DataOps + ModelOps + DevOps\nCI\/CD + Continuous Training\n0단계(수동) > 1단계(파이프 자동) > 2단계(CI\/CD 자동화)\n개발 > CI > CD > CT > CD > 모니터링\n\n디자인 > 트레이닝 > 운영"},{"토픽":"DataOps","키워드":"애 뎁 린\n샌 스 프(탐 초 분)\n오 메 인 보 모 변 테","도식":null,"정의":"기업 Data의 Insight 얻기 위해DevOps 접근법을 이용,Data Pipeline 자동화 위한Agile한 데이터 엔지니어링\n\n데브옵스 팀과 데이터 엔지니어, 데이터 과학자 역할을 결합해 데이터 중심 기업을 지원하는 도구, 프로세스, 조직체계","내용":"[애자일 + DevOps + Lean 제조] + Data 절차, 인력, 기술\n샌드박스(탐색) > 스테이징(초기 모델) > 프로덕션(분석)\n- 이 절차를 통해 Data pipeline 설계 및 자동화 수행\n오케스트, 메타 Data, 인증, 보안, 모니터링, 변경관리, test"},{"토픽":"ModelOps","키워드":"지 규 최 언 에\n\nC 환 챔 버 스 롤","도식":null,"정의":"기계 학습, 지식 그래프, 규칙, 최적화, 언어 및 에이전트 기반 모델을 포함한 광범위한 운영화된 인공 지능(AI) 및 의사 결정 모델의 거버넌스 및 라이프사이클 관리체계","내용":"지식 그래프 > 규칙 > 최적화 > 언어 및 에이전트 기반 모델\n\n핵심 기능 : 지속적 통합\/연속 전달(CI\/CD) 통합, 모델 개발 환경, 챔피언 도전자 테스트, 모델 버젼, 모델 스토어 및 롤백"},{"토픽":"적응적 머신러닝","키워드":null,"도식":null,"정의":"학습과 검증 둘로 딱 나누는것이 아니라,Data를 활용해 동작하면서도동시에 학습하게 하는 기술","내용":"적응형 AI"},{"토픽":"적응형 AI\n(Adpative AI)","키워드":"M 신딥 전메 유","도식":null,"정의":"변화하는 데이터와 변화하는 환경 모두에서 스스로 학습하고 적응하면서 스스로 개선하는 최신 형태의 인공 지능","내용":"머신 러닝 알고리즘 \/ 신경망 & 딥러닝 \/ 전이학습 & 메타러닝 \/ 유전자 알고리즘"},{"토픽":"AIOps","키워드":null,"도식":null,"정의":"IT 운영 최적화를 위해운영 Data를 분석,이상현상 및 반복 문제 등을식별하는 AI 솔루션","내용":"머신러닝, Big Data > Insight > 모니터링, 자동화, 서비스"},{"토픽":"인공지능 학습용 데이터구축비용 산정요소","키워드":"물 인 시 기 장","도식":null,"정의":null,"내용":"물량 : Data 양, 라벨링 작업량\n인력 : 참여 인원, 작업자 수\n기간(시간) : 구축 소요 기간, 장기\/단기\n활용 기기 : 구축 시 사용 장비, 저장소\/수집기\/분석기 등\n작업 장소 : 구축 작업 진행 공간, 임대\/임차 비용"},{"토픽":"인공지능학습용 데이터구축비용 구성","키워드":"인 제 직","도식":null,"정의":null,"내용":"직접 인건비 : 직접적 구축 비용(수집, 생성, 라벨링, 검수)\n제경비 : 간접 발생 비용(기획, 경영, 총무 등 부대 업무)\n직접 경비 : 직접 소요 비용(컨텐츠 구매, 장비료, 임대료)\n- 직접 경비 = (직접 인건비 * 115~120%)"},{"토픽":"인공지능학습용 데이터구축 절차","키워드":"수 획 정 가 학 운\n구 원 천 라 학","도식":null,"정의":null,"내용":"구축계획 수립 > 데이터 획득\/수집 > 데이터 정제 > 데이터 가공 > 데이터 학습 > 운영\n\n구측계획서 > 원시데이터 > 원천데이터 > 라벨링데이터 > 학습데이터"},{"토픽":"인공지능학습용 데이터구축 자료, 단위","키워드":"텍 이 음 영\n건 개 곳 문 쌍 장","도식":null,"정의":null,"내용":"자료 : 텍스트, 이미지, 음성, 영상\n단위 : 단일 요소(건, 개, 곳) \/ 복합 요소(문장, 쌍, 장)\n- 건(사건, 서류, 안건) \/ 개(낱 물건) \/ 곳(장소, 위치 단위)\n- 문장(NLP) \/ 쌍(질문-응답 등 set) \/ 장(사진, 음성 script)"},{"토픽":"인공지능학습용 데이터품질관리 가이드2.0","키워드":"역 대 생","도식":null,"정의":null,"내용":"1권 : 품질관리 안내서\n2권 : 데이터 구축 안내서\n\n역량부족 > 수행기관 품질관리 역량강강화\n품질관리체계미흡 > 인공지능 학습용 데이터 확보\n품질이슈 지속 개시 > 데이터 산업 생태계 지원"},{"토픽":"인공지능학습용 데이터품질관리 가이드3.0","키워드":"단 프 산 품\n계 수 획 정 가 학 운","도식":null,"정의":"기존의 1.0\/2.0을 품질관리 프레임워크 및 프로세스 기반으로 정립, 체계화\/고도화","내용":"프레임워크 : 단계 \/ 프로세스 \/ 산출물 \/ 품질관리 활동\n프로세스\n준비 : 구축 계획 수립\n구축(산출물) : 데이터 수집(원시), 획득, 정제(원천), 가공(라벨링), 학습(학습용)\n운영\/활용 : 데이터 운영,활용(공개용 학습용 데이터)"},{"토픽":"인공지능학습용 데이터특징","키워드":"학 기 비 원 파 ","도식":null,"정의":"빅데이터랑 비교해 정리","내용":"목적 : (학) AI 모델 학습 \/ (빅) 인사이트 도출, 비즈니스 개선\n기술 : (학) 분류, 예측, 군집, 축소 \/ (빅) 통계, Text 마이닝\n유형 : (학) 비정형 중심 \/ (빅) Text, 정형\n구조 : (학) 원천-라벨링 Data 쌍 \/ (빅) 키-값 구성\n저장 : (학) 파일형태(원천+JSON파일) \/ (빅) NoSQL, 키+값\n절차 : (학) 수 획 정 가 학 운 \/ (빅) 수집 > 정제 > 변환 > 저장"},{"토픽":"인공지능학습용 데이터구축 과정","키워드":"수 획 정 가 학 운\n준 구 운","도식":null,"정의":null,"내용":"구축계획수립 > 데이터 획득\/수집(원시) > 데이터정제(원천) > 데이터가공(라벨링) > 데이터 학습(학습용) > 데이터 운영\/활용\n- 정의 : 구축계획서 통해 해결 대상 문제 및 필요 Data 정의\n\n준비(수) > 구축(획,정,가,학) > 운영\/활용(운)"},{"토픽":"인공지능 학습용 데이터 품질관리 프로세스","키워드":"포 기 세 메","도식":null,"정의":null,"내용":"포맷 확인 > 기준(해상도 등) > 세부(서체 등) > 메타 정보\n기준 : 이미지 잘림, 식별 불가, 16kHz, 무음 구간 2초 이상,\n차별\/선정적 내용, 개인정보\/얼굴 비식별화, 중복영상 방지"},{"토픽":"어노테이션 & 라벨링","키워드":"라 엑 제 개 객 영\n어 속 형 설 주 바 폴 픽","도식":null,"정의":null,"내용":"라벨링 : XML, JSON 형식, 실제 분류\/참값(Ground Truth)\n- 단어 분류, 개체명 인식, 객체 인식, 영역 구분, 객체 추적\n어노테이션 : Data 속성, 파일형식, 설명, 주석\n- 바운딩 박스, 폴리곤, 픽셀, class 라벨, 키 포인트, 해상도"},{"토픽":"어노테이션,라벨링 기준","키워드":"세 클 입 검","도식":null,"정의":null,"내용":"세그멘테이션 : 하나의 seg 안에는 한자 한 글자만 포함\n클러스터링 : 하나의 cluster에는 동일 한자들만 있어야\n입력 : 이미지에 해당하는 정확한 한자 음\/훈 입력\n검증 : 정확히 일치하는 텍스트 입력, 동형이음은 통일"},{"토픽":"인공지능학습용 데이터품질관리 개요","키워드":"데 프 개\n원 관 생 개 조 교\n원 데 요 법 목 라 유","도식":null,"정의":null,"내용":"영역 : 데이터 자체 \/ 프로세스 \/ 개방 Data(민간 개방 Data)\n원칙 : 관리(全 생애 품질 보장, 지속 개선, 조직, 교육 지원), \nData(요구 충족, 법 제약 X, 목적 부합, 라벨링 정확, 유효성)\n모델, 체계(계 구 운 활, 품질 Cycle), 지표, 조직"},{"토픽":"인공지능학습용 데이터품질관리 활동","키워드":"수 획 정 가 학 운\n(구축 절차)","도식":null,"정의":"구축 절차에 맞춰서 품질관리 수행","내용":"계획 수립 : 수립, 절차\/조직구성, 적절성 검토, 품질 목표 및 점검 기준 수립\n획득\/수집 : 방법 및 기준 현행화, 법적근거 검토, 도구\/저장환경, 원시데이터 품질검사\n정제 : 방법 및 기준 현행화, 개인정보\/민간정보 비식별화, 도구\/저장환경, 원천데이터 품질검사\n가공 : 라벨링 방법 및 기준 현행화, 도구\/저장환경, 라벨링 데이터 품질검사\n학습 : 합치성 확인, 결과 확인\/최적화, 품질검증 보완조치\n운영\/활용 : 하자 및 유지보수, 품질개선 의견 반영"},{"토픽":"운영\/활용 단계 품질검사","키워드":"정 품 분 개 통","도식":null,"정의":null,"내용":"검사대상 정의 > 품질검사 수행 > 검사결과 분석 > 개선 수행 > 품질통제\n정의 : 계획 검토 및 후보 데이터 선정, 데이터 선정, 계획 수립\n질검사 : 준비, 실시\n분석 : 분석, 개선기회 도출\n개선 : 개선 계획 수립, 실시\n통제 : 평가, 관리, 실시"},{"토픽":"1 Cycle 중심 검사","키워드":"수 가 참 구\n\n배 작 품 조 제 적","도식":null,"정의":"초기 단계 Data를빠르게 AI 모델에 적용해보고실질적 유효성 확보 가능한지확인하는 절차* 초기 Data : 전체의 10% 미만\n\n구축 협약 이후 1~3개월 이내 샘플, 초기 데이터 대상 전체데이터의 10% 이내의 데이터로 품질에 대한 미흡점과 개선점을 점검하는 프로세스","내용":"유효성 목표 달성 어려워보일 시 빨리 수집\/정제 절차 변경\n수집\/정제 > 가공 > 참값 검증 > 구조 검수\n- 초기 뿐 아니라 사업 수행 중에도 지속 수행\n- 수집\/정제 : 전수 \/ 파일명 규칙, 포맷, 유효성, 메타 항목\n- 가공 : 전수 \/ 누락 여부, 메타 완전성, 전사원칙 준수\n- 참값 : 샘플 \/ 상시검증 Lv 1, 2, 3\n- 구조 : 전수 \/ 구조, 형식, 통계 검수\n\n자가점검 체크리스트 양식배포 > 체크리스트 작성 > 품질개선 여부 확인 > 미흡항목 점검 및 조치 > 과제조정위원회 자가점검 체크리스트 제출 > 결과물 적정성 검토"},{"토픽":"인공지능학습용 데이터품질관리 지표","키워드":"다 구 의 유","도식":null,"정의":null,"내용":"구축 공정 : 준비, 완전, 유용성\nData 적합성 : 기준 적합, 기술 적합, 통계적 다양성, 요건 다양성\n- 기준 : 다양, 신뢰, 충분, 사실성 측정\n- 기술 : 파일포맷, 해상도, 선명도, 컬러, 크기, 길이, 음질\nData 정확성 : 의미 정확성, 구문 정확성\n- 의미 : 라벨의 정확도, 정밀도, 재현율 측정(실제 참값 비교)\n- 구문 : 라벨의 속성 누락, 속성 Data 값 누락, 형식 준수도\n유효성 : 알고리즘 적정성, 학습 모델 유효성"},{"토픽":"인공지능학습용 데이터품질관리 프레임워크","키워드":"단 프 산 관","도식":null,"정의":null,"내용":"단계 : 준비\/계획 > 구축 > 운영\/활용\n프로세스 : 준비\/계획, 구축 및 운영\/활용 단계별 구축 공정 프로세스\n산출물 : 공정 프로세스별 구축 및 품질 관련 산출물\n품질관리 활동 : 관리\/검증 활동, NIA 품질 관련 활동"},{"토픽":"확률분포","키워드":"이 베 이 포 질\n연 정 표 티 엪 카 밀","도식":null,"정의":"주사위의 각 눈에 대한확률의 분포와 같이,확률변수가 특정값 가질 확률나타내는 함수","내용":"이산 : 베르누이, 이항(베르 반복 시 k번 성공 확률), 포아송\n* 확률질량함수 : 이산확률변수가 특정 값을 가질 확률\n\n연속 : 정규(통계량이 평균), 표준정규, t, f, 카이(분산)\n* 확률밀도함수 : 연속확률변수가 특정 구간에 포함될 확률"},{"토픽":"베르누이 분포","키워드":"0 1","도식":null,"정의":"나올 수 있는 경우의 수가2개밖에 없는Binary Value에 대한 분포","내용":"베르누이 시행 : 2가지 상황만 나오는 실험\n베르누이 확률변수 : 시행 결과를 실수 0 또는 1로 바꾼 것\n베르누이 분포 : 베르누이 확률변수의 분포\n기댓값 E(X) = p   \/   분산 Var(X) = p(1-p)\n* x값은 0, 1만 있으므로 성공 시 1, 실패 시 0 대입"},{"토픽":"이항 분포","키워드":"np \/ np(1-p)","도식":null,"정의":"베르누이 시행을 n번 수행 시성공한 횟수(x)를확률변수로 하는 분포","내용":"확률변수(X) : N번의 베르누이 시행에서 성공의 횟수\n- 5번 시행 중 3번 성공이면 n = 5, x = 3\n평균 E(X) = np   \/   분산 Var(X) = np(1-p)"},{"토픽":"포아송 분포","키워드":"λ","도식":null,"정의":"일정한 시간\/공간 단위에서발생하는 이벤트 수를확률변수로 하는 분포","내용":"이항분포에서 n이 너무 크고 p가 너무 작을 시의 문제 해결\n1시간 동안 한 사람이 넘어질 가능성은?\n- 1000명 중 10초에 1명 넘어짐 > 10초에 0.1% 로 넘어짐\n- 3600초 간 10초에 0.1%로 일어나는 사건이 360번 일어남\n평균 = λ   \/   분산 = λ"},{"토픽":"정규분포","키워드":"μ σ","도식":null,"정의":"평균(μ)과 분산(σ^2)에 따라분포와 모양이 결정되는좌우대칭 종 모양 연속확률분포","내용":"μ : 분포의 중앙 \/ σ : 분포의 퍼짐 정도(클수록 퍼짐)\n양쪽 3표준편차(μ±3σ) 안에 99.74%의 관측치가 포함\nx축이 점근선, 곡선 아래 면적 1, 최빈\/중앙\/평균 같음\n* 3단락 : 중심극한정리(포본 평균 분포는 정규) or Z 분포"},{"토픽":"표준정규분포\n(Z 분포)","키워드":"모 평 표 편","도식":null,"정의":"서로 다른 확률분포 비교 위해,평균, 분산, 측정단위에영향받지 않는표준화 된 정규분포","내용":"모집단의 평균\/표준편차 알 때 씀\n표본의 표준편차 : 모표준편차 \/ 루트 N(표본 개수)\n표본의 수가 30 개 미만일 경우 Z분포 이용 불가 > t 분포 씀"},{"토픽":"Z 검정","키워드":null,"도식":null,"정의":"모집단의 평균\/표준편차 알 때,새롭게 조사된 표본의 평균이모집단 평균과 같은지추정하는 검정","내용":"Z 분포 이용해서 검정"},{"토픽":"확률 변수 적률","키워드":"평 분 왜 첨","도식":null,"정의":"Data의 학습효과 예측 위해함수의 모양을수학적으로 표현하는하나의 척도","내용":"Outlier 비율 보고 학습효과 예측해 더 잘 학습하려고 측정\n- 1차 적률 : 평균 \/ 2차 적률 : 분산\n- 3차 적률 : 왜도 \/ 4차 적률 : 첨도\n* 첨도가 너무 높으면 Outlier 좀 없앤다든가 해서 대응"},{"토픽":"왜도\n(Skewness)","키워드":"모 미 민\n왜 양 우 꼬","도식":null,"정의":"중심으로부터분포가 기울어진 방향, 정도를나타내는 척도","내용":"S > 0 : Mode < Median < Mean\n왜도 양수면 우측 긴 꼬리, 음수면 좌측 긴 꼬리\n* Skew 높으면 꼬리부분 학습 미흡\n> Square Root, Cube Root, Log, Outlier 제거 등 정규화"},{"토픽":"첨도\n(Kurtosis)","키워드":3,"도식":null,"정의":"중심값을 기준으로Data가 집중적으로 분포된,자료 분포의 뾰족한 정도","내용":"K = 3 : 표준정규분포와 뾰족한 정도가 같음\nK > 3 : 표준정규분포보다 뾰족함\nK < 3 : 표준정규분포보다 납작함\n* Outlier 많으면 첨도 높음\n\n보통 3을 기준으로 표준 정규분포와의 차이 확인하지만, -3 보정하여 음\/양으로 구분하기도 함"},{"토픽":"표본 추출","키워드":"2 층 집 계 단\n눈 할 판 임","도식":null,"정의":"모집단 특성 추론 위해모집단에서일부 샘플 추출하는 기법","내용":"확률적 : 모집단에서 표본이 추출된 확률을 앎, 표집틀 O\n- 단순랜덤, 층화(학년별 랜덤), 계통(k 간격), 집락(군집)\n+ 2단계 : 10세트 중 3세트 추출 후 3세트 Data 중 랜덤 추출\n비확률적 : 모집단에서 표본이 추출된 확률을 모름, 표집틀 X\n- 눈덩이, 할당(랜덤 X), 판단(유의, 잘 알 때), 임의(편의)"},{"토픽":"중심극한정리\n(CLT,Central Limit Theorem)","키워드":"30 부트스트랩\nz 1.96","도식":null,"정의":"모집단에서 크기 n, 표본 k개복원 추출 시 n, k 충분히 크면각 표본 평균의 분포는정규 분포로 수렴한다는 이론","내용":"표본크기(추출 횟수) 30 이상\n> 평균 μ, 표편 σ\/√n 인 정규분포로 수렴\nbootstrap이 통계적으로 타당하다는 근거\n\n그래서 z검정도 표본이 30개면 1.96으로 검정값 통일 가능"},{"토픽":"데이터 전처리","키워드":"정 통 변 축","도식":null,"정의":"데이터 정제, 통합, 축소 등으로결측\/이상값 제거 및Data 분석 정확도 향상 등 위한처리 절차","내용":"정제 : 결측\/이상\/Noise 제거(ETL, 맵리듀스, Spark, Flume)\n통합 : 여러 Data set 호환 가능하게 통합, 중복 제거\n변환 : Data 형식\/구조 변환(정규\/집합\/평활화\/요약)\n축소(정리) : 효율적인 Data 표현(PCA, 차원축소, 샘플링)"},{"토픽":"이상값\n(Outlier)","키워드":"원 입 측 실 표 고\n검 관 통 시 군 마 롶\n처 삭 대(평회핫다) 변 분","도식":null,"정의":"일정한 범위에서 벗어난,특이하게 작거나 큰 Data 값","내용":"Inner(1.5*IQR)-Mild \/ Outer Fence(3*IQR)-Extreme\n원인 : 입력\/측정\/실험\/표본추출 오류, 고의적 이상값\n검출 : 관찰, 통계, 시각화, 군집화, 마할라노비스, LOF\n* LOF : Data 전체 아닌 주변(Local)만 갖고 Outlier Factor\n처리 : 삭제, 대체(평균\/회귀\/핫덱\/다중), 변환(로그), 분류\n- 핫덱 : 해당 자료의 가장 많은 값으로 대체\n- 콜드덱 : 타 자료의 가장 많은 값으로 대체\n* Outlier 많으면 첨도 높음"},{"토픽":"결측값","키워드":"완 무 비\n제 단 다 예\n대 분 결","도식":null,"정의":"실험\/조사 시관측되어야 할 값을 얻지 못해,데이터 전처리 시적절 값으로 처리하는 기법","내용":"완전 무작위 : 측정 대상 특성과 상관 없는 결측(단순 누락)\n무작위 : 여성이 체중에 미응답(다른 특성이 체중 결측 영향)\n비무작위 : 체중 높으면 미응답(해당 특성 자체로 인한 결측)\n제거 \/ 단일 대체 \/ 다중 대체 \/ 예측 모델(패턴, 회귀)\n- 다중대치 : 단일 대치 반복, 여러 set 생성 > 분석 > 결합"},{"토픽":"Data 필터링","키워드":"학 베 내 칼\n비 질 해 블","도식":null,"정의":"데이터 오류 또는저품질 Data 제거하여저장 공간 및 분석 시간을효율화하는 기법","내용":"학습 기반 : 필터링 수행 전 필터 모델 학습\n- 베이지안 : 시간별 측정값과 행동 기반, 상태(노이즈) 유추\n- 내용 기반 : item\/user profile 간 유사성 기반 필터링\n- 칼만 : 베이지안 활용(과거 Data 기반 현 Data 노이즈 제거)\n비학습 기반 : 필터링 학습 없이 필터링 수행\n- 질의 : 정형 Data 기반, select 질의 결과 처리\n- 해시 : 컨텐츠 해시를 DB에 저장해서 해시 일치 여부 비교\n- 블룸 : 해시로 블룸 필터 확인, False positive 가능\n* 블룸 필터 : 원소가 집합에 속하는지 검사하는 자료구조"},{"토픽":"베이즈 필터링","키워드":"측 행 상","도식":null,"정의":"과거 사건(사전확률) 기반,사후 확률 계산하여필터링 대상을 스스로 학습해제거하는 기법","내용":"과거에 A 발생 시 B로 분류했다면, 다음에도 그럴 확률 높다\n> 텍스트 분류에 적용해서 스팸 메일 필터링에 자주 씀\n시간 및 측정 업데이트식이 번갈아 작동하는 recursive 필터\n0~t까지 측정값 Z랑 행동 U의 시퀀스로 상태변수 X 추정"},{"토픽":"베이즈 정리","키워드":"앱 = 비 분의 빠","도식":null,"정의":"사전 확률을 기반으로사후 확률을 구하는 기법","내용":"고전 통계라면 내일 해가 뜰 확률은 2분의 1이나,\n베이지안은 맨날 떴으니까 내일도 뜰 확률 99.99퍼라고 함"},{"토픽":"베이즈 정리구성요소","키워드":"전 우 후","도식":null,"정의":null,"내용":"A1~An 중 하나는 꼭 일어날 때, B땜에 A 일어날 조건부 확률\n- 사전확률 : P(Ai), 관측자가 이미 알고 있는 사건의 확률\n- 우도 : P(B|Ai), 이미 아는사건 발생 시 다른사건 발생 확률\n- 사후확률 : P(Ai|B), 사전확률\/우도 통해 아는 조건부 확률"},{"토픽":"Data 정규화\n(Normalization)","키워드":null,"도식":null,"정의":"범위가 다른 변수 간 비교 위해Data의 범위를 왜곡 없이공통 척도로 변경하는 기법","내용":"Min Max Scaling이 가장 대표적\n- X의 Scaling 값 = (X - Xmin) \/ (Xmax - Xmin)\n* 정규화, 표준화 모두 경사하강법의 빠른 적용 지원"},{"토픽":"배치 정규화","키워드":null,"도식":null,"정의":"빠른 학습을 위해학습\/추론 단계에서Batch 간 분포를 통일,평균 0 \/ 분산 1로 만드는 활동","내용":"전체 Data는 Epoch > Batch로 나눔(SGD는 Batch 기반)\n- Batch 간 분포 차이 발생 가능 > 평균 0 \/ 분산 1로 정규화\n- 학습 단계 : Batch마다 평균\/분산 계산해서 적용\n- 추론 단계 : 학습 단계의 평균\/분산 값 저장했다가 적용"},{"토픽":"Data 표준화\n(Standardizing)","키워드":null,"도식":null,"정의":"Data가 표준정규분포가 되도록평균이 0, 표준편차가 1이 되는Feature Scaling 하는 기법","내용":"Z = (X - μ) \/ σ"},{"토픽":"TFDV\n(Tensorflow Data Validation)","키워드":"아 스 드","도식":null,"정의":"TFX(Extended)에서데이터 전처리 전에Data가 garbage인지 아닌지검증할 때 쓰이는 라이브러리","내용":"정형 Data에 특화됨, 주피터를 통해 시각화 기능을 제공\nanomal > skew > drift 데이터 찾기\nanomal : 잘못된 데이터가 들어왔는지 검증, skew : 두 Data\n셋의 분포가 얼마나 다른지, drift : 과거 Data 분포와 비교"},{"토픽":"통계학4가지 척도","키워드":"명 서 등 비","도식":null,"정의":"대상의 정성적 특성 등을정량적 수치로 표현하는 단위","내용":null},{"토픽":"데이터 구분","키워드":"범명순 수이연","도식":null,"정의":"Data를 형태, 구조 등에 따라분류한 구분","내용":"형태 : 범주형(명목, 순서) \/ 수치형(이산, 연속 or 등간\/비율)\n구조 : 정형, 반정형, 비정형"},{"토픽":"척도 기법","키워드":"리 의 거 보 서","도식":null,"정의":null,"내용":"리커트 : 서열(5점, 7점 등) \/ 전혀 - 다소 - 보통 - …\n의미분별 : 반대 단어 사이 선택 \/ 보수적 1 2 3 4 5 진보적\n거트만 : 태도 측정, 이 항목 동의 시 다른 항목도 동의 함의\n보가더스 : 서열 \/ 사회적 거리감 측정(소수민족, 계급 등, 거트만 척도의 한 종류)\n서스톤 : 등간 \/ 여럿이 문항 평가, 타당 높은 문항이 곧 척도"},{"토픽":"통계","키워드":"기 추(점 구 가 차)","도식":null,"정의":null,"내용":"기술통계 : 있는 그대로 표현\n추론통계 : 점추정, 구간추정, 가설 검정, 차이 검정"},{"토픽":"기술통계\n(Descriptive Statistics)","키워드":"일 이\n평 빈 범 표 백 사","도식":null,"정의":"모집단에서 표본 추출 후해당 표본의 중간값, 산포,빈도수, 그래프 등기술하는 통계(EDA)","내용":"일원적 : 단일 변수에 대한 분석(빈도분석)\n이원적 : 두 변수 간 관계(교차분석, t test)\n기술 : 평균, 최빈값, 범위, 표준편차(산포), 백분위, 사분위수"},{"토픽":"추론통계","키워드":"모 비 정\n점 구 가 차","도식":null,"정의":"기술 통계 기반으로모집단 특성을 추정하는추정 통계","내용":"모수적 : 정규분포를 가정하고 검정하는 방법\n비모수적 : 정규분포 가정 불가한 경우(표본 수가 적다든가)\n점 추정, 구간 추정(모수 있을 구간 정하고, 모수가 실제 해당\n구간에 있을 확률), 가설 검정, 차이 검정(집단 간 차이 비교)"},{"토픽":"추론 통계 조건","키워드":"불 효 일 충","도식":null,"정의":"모수에 대한 올바른 추정 위한불편의, 효율, 일치, 충분성","내용":"불편 : 추정량 기대값과 모수가 편차 없이 동일(표본평균)\n효율 : 여러 추정량 중 분산이 적은 추정량\n일치 : 표본크기 무한히 증가할 때, 추정량은 모수에 근접\n충분 : 추정량은 모수에 관한 모든 정보를 포함해야 함"},{"토픽":"가설","키워드":"단 복","도식":null,"정의":"모집단의 특성을 대표하는모수에 대한 주장이나 예상","내용":"단순가설 : 확률분포 완전 결정하는 가설\n- ex) 평균과 분산이 각각 0, 1이라고 가정\n복합가설 : 확률분포를 완전히 결정하지 않는 가설\n- ex) 평균이 1보다 클 것이다"},{"토픽":"귀무가설","키워드":"P 유 보 크 귀 채","도식":null,"정의":"대립가설이 참이라는확실한 근거가 없을 시받아들이는 가설","내용":"실제 검정 대상(기각역에 속하냐 아니냐)\np값이 유의수준보다 크면 귀무가설 채택\n유형 : 단순가설, 복합가설\n모수가 특정 값이다 \/ 두 모수 값이 같다, 등 간단\/구체적"},{"토픽":"대립가설","키워드":null,"도식":null,"정의":"표본으로부터확실한 근거에 의해입증하고자 하는 가설","내용":"검정 대상 아님, 귀무가설 기각 시 자동 수락\n검정 유형 : 방향성 가지면 단측검정, 방향성 X면 양측 검정"},{"토픽":"혼동행렬","키워드":"확 밀 민 위 부 특 음\n\n목적지 \/ 목적지 + 출발지","도식":null,"정의":"Training 을 통한 Prediction 성능을 측정하기 위해 예측 value 와 실제 value 비교로 이진 분류기의 성능을 시각화한 매트릭스모델의 예측값의 실제 일치 여부를 판단하기 위하여 행렬 형태로 시각화한 모델 평가 기법혼동행렬 암기법 : https:\/\/zetawiki.com\/wiki\/%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%AC_%EA%B8%B0%EC%96%B5%EB%B2%95","내용":"위에 실제 \/ 왼쪽 예측(예시그림)\n측정기준 : TP, TN(예측이 정확) \/ FP, FN(예측이 틀림)\n위에 예측 \/ 왼쪽 실제면 : TP FN TN FP 순으로 시계방향\n주요지표\n정확도 : (TP + TN) \/ (TP + TN + FP + FN)\n정밀도 \/ 양성예측도(PPV) : TP \/ (TP+FP)\n민감도(TPR) : TP \/ (TP + FN)\n위양성율(FPR) : FP \/ (FP + TN)\n부정오류율(FNR) : FN \/ (FN + TP)\n특이도(TNR) : TN \/ (TN + FP)\n음성예측도(NPV) : TN \/ (TN+FN)\n\n양극평균 : (TPR+TNR) \/ 2\nF1 Score : 2 * (TPR*PPV) \/ (TPR+PPV)\nKappa : accuracy - P(e) \/ 1-P(e)"},{"토픽":"딥러닝평가지표","키워드":"X : FPR\nY : TPR","도식":null,"정의":"학습 중 및 학습 완료된딥러닝 모델의 성능 확인 및 평가 위한 지표","내용":"예측모델 : 비용함수 기반\n분류\/군집 : ROC, Lift 차트, 정확도, Precision, Recall, F1"},{"토픽":"분류 \/ 군집 모델평가 수식","키워드":"어 프 리(팁 픈) 알 실\n엪 이 재 정","도식":null,"정의":null,"내용":"혼동 행렬 : Accuracy, Precision, Recall(TP \/ TP+FN)\nROC 커브(TPR, FPR, AUC)\n실루엣 계수(군집, 거리기준)\nF1 score : 2*재현율*정밀도 \/ (재현율 + 정밀도)"},{"토픽":"EDA\n(ExploratoryData Analysis)","키워드":"수 패 가(데>모)\n저 잔 재 현\n히 줄 상 산","도식":null,"정의":"가설 도출을 위해,가공 안한 Data로전체 특성을 파악하는탐색적 통계분석 기법","내용":"RawData 수집 > 패턴 > 가설 도출(Data > 모형)\n핵심 요소 : 저항성, 잔차해석, 자료의 재표현\/현시성\n히스토그램, 줄기잎그림, 상자수염그림, 산점도\nEDA로 모형 > CDA 유의성 증명 > PDA(예측)로 분석\/최적화"},{"토픽":"CDA\n(Confirmatory Data Analysis)","키워드":"정 측 분 개 검\n티 엪 아 상 회 카","도식":null,"정의":"가설 검증을 위해,구체적 가설에 입각한통계적 추론 기법","내용":"P-value 기준 수용\/기각 결정\n정의 > 측정 > 분석 > 개선 > 가설 검증(모형 > Data)\n핵심 요소 : 중심극한의 정리, P-value(유의확률)\nt-test, F-test, ANOVA, 상관\/회귀\/카이제곱\/모비율"},{"토픽":"PDA\n(PredictiveData Analysis)","키워드":null,"도식":null,"정의":"CDA로 나온 가설로 관계식을 만들고,최적 조건 설정 및성과 예측하는 기법","내용":"CDA로 가설 검증 > 검증된 가설 활용하여 미래를 예측\n최소 제곱법, 비선형, 신경망, 가우시안, 시뮬레이션"},{"토픽":"통계적가설 검정 절차","키워드":"가, 검, 유, 기, 검계, 검기속\n(간단 버전 : 가 검 유 결)","도식":null,"정의":"통계적 추론의 하나로,모집단에 대한 가설을표본 정보를 사용해합당성 여부를 판정하는 과정","내용":"가설(H0, H1) 설정 > 검정통계 유형 결정 > 유의수준 설정 >\n유의수준에 해당하는 기각역 설정 > 표본수집, 검통 계산 >\n검통이 기각역에 속하나? > 속하면 H0 기각, 안 속하면 채택\n* 검통 : Z, t, 카이(X^2), F"},{"토픽":"통계적가설 검정 오류","키워드":"1 : 귀 실 참 채 않 오\n2 : 귀 실 거 채 오","도식":null,"정의":null,"내용":"1종(α, 기각역) : 귀무가설이 실제 참인데 채택하지 않는 오류\n- False Positive : P의 작대기 1개라 1종 오류\n2종(β, 채택역) : 귀무가설이 실제로 거짓인데 채택하는 오류\n- False Negative : N의 작대기 2개라 2종 오류\n신뢰수준(신뢰도) : α(유의수준) \/ 검정력 : 1- β"},{"토픽":"통계적가설 검정 분류","키워드":"평 페 독 아\n분 카 엪\n가 독 정 등","도식":null,"정의":null,"내용":"검정 대상 기준으로 분류\n- 평균(모집단 1개 : Paired T \/ 2개 : 독립 T \/ 3개 : ANOVA)\n- 분산(모집단 1개 : 카이제곱 \/ 2개 : F검정)\n가정 : 독립성(독변), 정규성(Y 분포), 등분산성(Y 분포)"},{"토픽":"T-test","키워드":"티 독 명 종 연","도식":null,"정의":"모집단 분산 모를 시평균 간의 차이가유의한 차이인지표본의 표준편차로 검정 기법","내용":"Paired T : Data가 하나 모집단에서 반복 추출된 경우\n독립표본 T : Data가 두 모집단에서 추출된 경우\n자유도 : 표본수(n) - 1 \/ 독립 : 명목, 종속 : 연속\n* P-value가 0.05보다 작으면 유의 \/ t값 클수록 유의"},{"토픽":"t 분포","키워드":null,"도식":null,"정의":"정규분포보다 평평\/두텁 꼬리모평균\/모표준편차 모르고표본 크기 작을 때(30 미만)모평균 추정에 이용하는 분포","내용":"정규모집단으로부터 크기 n인 표본 무작위 추출 시\n표본 통계량 t는 자유도 (n-1)인 t 분포를 따름\n자유도 ↓수록 Z분포보다 평평, ↑수록 분산 1에 접근"},{"토픽":"ANOVA\n(분산분석, F검정)","키워드":"독 정 등\n튜 던 엘 셰(사후검정 기법)","도식":null,"정의":"3개 이상 집단 간F값 기반의평균 차이 검정 방법론(X는 명목, Y는 연속)","내용":"조건 : 독립성, 정규성, 등분산성(집단별 Levene 검정)\nF값 = (집단 간 \/ 집단 내), 클수록 유의(P, 유의수준 활용)\n한 그룹만 평균 차이 있는지, 모든 그룹 차이 있는지 확인불가\n> Tukey : 표본수 동일 \/ Duncan : 엄격X \/ LSD : 가장 유연\nScheffe : 가장 엄격, 유의한 차이 도출 어려움"},{"토픽":"ANOVA 유형","키워드":"원 반 투 웨 멀","도식":null,"정의":null,"내용":"One way : 한 개의 집단 구분 독립 변수 분석\nRepeated Measure : 집단에 대한 반복 측정 분석\nTwo way : 두 개의 집단 구분 독립 변수 분석\nMulti way : 다수의 집단 구분 독립 변수 분석\nMultivariate : 한 개의 집단 구분 독립 변수와 두 개 이상의 종속변수 분석"},{"토픽":"카이제곱 검정\n(교차 분석)","키워드":null,"도식":null,"정의":"범주형 자료 간어떤 영향을 미치는지교차표 기반으로 분석하는 방법(X, Y 둘 다 명목)","내용":"X, Y 모두 범주형(X : 집단 \/ Y : 어떤 특성의 비율)\n예시 : 성별에 따라 선호하는 커피브랜드의 차이가 있는가?\n\n관찰값 Nxy = O\n기대값 Exy = E"},{"토픽":"상관 분석","키워드":"선 등\n피 스 켄","도식":null,"정의":"두 변수간의 상관계수를 통한데이터 안의 선형 관계에 대한 통계적 분석 기법(X, Y 둘 다 연속)","내용":"조건 : 선형성, 등분산성(x에 따른 y의 흩어진 정도 동일)\n\n상관계수\n피어슨 : 모수 검정, 연속 - 연속, cov(X, Y) \/ Sx Sy\nxy공분산\/x표준편차*y표준편차\n\n스피어만 : 비모수 검정, 서열 - 서열(순위), 순서척\n1-6∑di^2\/n(n^2-1)\n\n켄달(τ, 타우) : 서열, (A, B) 키가 (1, 2), 무게(3, 4)면 C\n(A, C) 키가 (1, 3), 무게(3, 1)이면 D\n- τ = (C-D) \/ (C+D)      C는 concordant, D는 아닌거"},{"토픽":"머신러닝","키워드":"데 요 딥","도식":null,"정의":"인공지능의 한 분야로필요 특징을 사람이 선택 후Train Data로 학습해예측 및 자동 개선하는 기법","내용":"구성 : 데이터(입력), 요인 추출, (딥)러닝 모델\n- 요인 추출 : 인풋 과도 방지, 높은 정답률 갖는 핵심만 추출\n- 딥러닝 모델 : 답(label) 찾아가는 함수"},{"토픽":"Feature Engineering","키워드":"선 단 조 의\n추 피 엘\n생 도 전 경\n\n이 정 비 개 구","도식":null,"정의":"모델의 성능을 높이기 위해주어진 초기 데이터의 특징을가공하고 생성하는 공학 기법","내용":"선택 : 회귀모델(전진\/후진\/단계), 모든 조합, 의사결정 나무\n추출 : PCA, LDA(클래스 내\/간 분산 비율 최대화)\n생성 : 도메인 지식 바탕, 전문성, 경험\n피처이해 : 데이터 목표 생성 \/ 사전평가자료생성 \/ 탐잭적 데이터분석 \/ 타겟변수 선택\n정형 데이터\n피처개선 : 데이터 변환 \/ 틀린점 \/ 스케일 조정\n피처구성 : 피처 추출 \/ 선택\n비정형데이터\n피처개선 : 데이터 변환 \/ 틀린점 \/ 스케일 조정\n피처구성 : 추출 \/ 선택"},{"토픽":"단층 퍼셉트론","키워드":"입 출 활 단","도식":null,"정의":"입력, 출력층만 갖고입력에 가중치를 곱한 값을활성화 함수로 Output 출력하는단층 인공신경망","내용":"Output = F(w0 + w1Input1 + w2Input2…) \/ F = 활성화함수\n- Input과 Output을 모두 알면, w를 찾아내는 것이 가능\n- 이를 통해 weight가 학습되면, input에서 output 산출 가능\n> But, 일차 방정식은 선형이므로 XOR 을 분리할 수 없음"},{"토픽":"다층 퍼셉트론\n(MLP)","키워드":"입 은 출","도식":null,"정의":"단층 퍼셉트론 XOR 해결 위해은닉층을 1개 이상 둔퍼셉트론","내용":"MLP 학습 방법이 없어 인공신경망 침체기 있었으나\n오류 역전파로 다시 활성화 됨\n(순전파 > 역전파 > W 업뎃 > 순전파.. 반복시 오차 0 수렴)"},{"토픽":"심층신경망\n(DNN)","키워드":"은 은","도식":null,"정의":"2개 이상의 은닉층을 가져다양한 비선형적 관계를학습할 수 있는 모델","내용":"RNN : 시계열 데이터 처리\nCNN : 2차원 데이터(이미지 등) 처리\nDBN(심층 신뢰), Deep AutoEncoder : 비지도 학습 기반"},{"토픽":"딥러닝","키워드":null,"도식":null,"정의":"머신러닝의 한 분야로'특징'을 선택하는 부분까지모델이 스스로 학습하여지도\/비지도 학습 등 수행 기법","내용":"이미지 처리 등은 pixel 별 RGB도 들어가다보니 인풋 많음\nDNN, CNN, RNN 등"},{"토픽":"경량 딥러닝","키워드":"경 구 필 자\n알 압 증 가 자","도식":null,"정의":"딥러닝의 부하 감소 위해알고리즘 or 파라미터 등효율화하는 기법","내용":"경량 알고리즘 : 알고리즘 구조 자체가 가벼워짐\n- 모델구조 변경, 합성곱 필터 변경, 자동 최적 모델 탐색\n알고리즘 경량화 : 만들어진 모델 압축(파라미터 줄임)\n- 모델 압축, 지식 증류, HW 가속화, 모델 압축 자동 탐색\n* On Device AI 위해서는 경량 딥러닝 필요"},{"토픽":"하이퍼파라미터","키워드":"학 은 랜 활\n매 그 랜 베 진","도식":null,"정의":"머신러닝에서,최적의 모델링을 위해 사용자가직접 설정해주는 변수","내용":"학습률, 은닉층 개수, 랜덤 포레스트 나무 수, 활성화함수\n수동 : 매뉴얼(휴리스틱, 사용자 경험 기반)\n자동 : 모델 Free(Grid, Random) \/ 모델(베이지안, 진화형)\n- Grid : 모든 조합 전수 조사\n- 랜덤 : 최소~최대 범위 내 랜덤하게 조사, 연산 감소\n- 베이지안 : 이전 관측값 기반, 베이지안 모델 기반"},{"토픽":"파라미터","키워드":"가 서 결 편","도식":null,"정의":"머신러닝에서,주어진 Data로부터오류 역전파 등 학습 통해모델 내부에서 결정되는 변수","내용":"NN 가중치, SVM 서포트벡터, 선형회귀 결정계수, 편향"},{"토픽":"하이퍼 파라미터와파라미터 차이","키워드":"방 시 예 특","도식":null,"정의":null,"내용":"방법 : (파) 모델이 튜닝, 오류 역전파 \/ (하) 사람, Grid 방식\n시점 : (파) 학습 중, 딥러닝 시 \/ (하) 학습 전, 딥러닝 전\n예시 : (파) 회귀 계수, 가중치 \/ (하) 활성화 함수, 은닉층 수\n특징 : (파) 직접 조정 불가, 손실함수 \/ (하) 직접 수정, 경험"},{"토픽":"활성화함수","키워드":"전 비 깊\n시 계 렐 소 \/ 탄 맄 항","도식":null,"정의":"인공 신경망에서각 뉴런의 출력 신호 결정 위해입력*가중치에 적용하는출력층 함수","내용":"선형분류기 XOR 문제 해결 위해 나온 비선형 함수\n- 필요성 : 뉴런값 전달 여부 결정, 비선형 해결, 깊은 은닉층\n- 단층 퍼셉트론 : 계단함수 > 미분 불가\n- 근데 신경망은 가중치 학습 때 미분 사용 > ∴ 시그모이드\n- 단극성 : Sigmoid, 계단 함수 : y = 1(x ≥ 0), y = 0(x＜ 0),\nReLU : y = max(0, x), 소프트맥스\n- 양극성 : 탄젠트(-1~1), Leaky ReLU, 항등함수(y = x)"},{"토픽":"시그모이드 함수","키워드":null,"도식":null,"정의":"이진 분류에서 주로 사용하며주로 출력층에서만 쓰는단극성 활성화 함수","내용":"은닉층에서 사용할 시 기울기 소실 등 문제 발생\n> ReLU로 기울기 소실 해결\n출력값 : 0~1사이로 정규화 \/ 0.5 기준으로 이진 분류"},{"토픽":"소프트맥스 함수","키워드":null,"도식":null,"정의":"N개의 다중 Class 분류 위해N차원 벡터 입력받아각 Class에 속할 확률 추정하는단극성 활성화 함수","내용":"출력값 : 0~1 사이로 정규화 \/ 값의 총합은 항상 1\n각 클래스에 대한 확률값"},{"토픽":"하이퍼볼릭탄젠트 함수","키워드":null,"도식":null,"정의":null,"내용":"Sigmoid의 Non Zero Centered를 Zero로 맞춤"},{"토픽":"오류 역전파","키워드":"초 코 역 튜","도식":null,"정의":"다층 퍼셉트론에서출력값과 실제 값 간 오차를입력\/은닉층으로 역으로 보내파라미터(w) 최적화하는 기법","내용":"가중치(w) 초기값 > Cost 계산 > 오류 역전파 > 가중치 튜닝\n- 초기 파라미터(w)는 랜덤 설정\n- Cost(Loss) : Output-실제 간 오차"},{"토픽":"기울기 소실문제","키워드":null,"도식":null,"정의":"인공지능 2차 혹한기,역전파 알고리즘에서처음 입력층으로 진행될수록기울기가 0으로 수렴하는 문제","내용":"Layer가 많아질수록 기울기 작아져 학습 잘 안됨\nSigmoid는 0에서 기울기(미분값) 가장 큼 : 0.25\n- 0 근처 외엔 기울기 거의 0에 가깝 > 거꾸로 곱할수록 감소\n> tanh은 좀 더 완만해서 Sigmoid 보단 낫지만 여전히 문제\n> ReLU는 양수면 기울기 1, 음수면 0 > 소실 X, 연산 빠름\n> 입력 음수면 뉴런 회생 어렵(Dying ReLU) > Leaky ReLU"},{"토픽":"비용함수\n(손실함수,목적함수)","키워드":"제 루 절 바 카 스","도식":null,"정의":"가중치별신경망 예측값과 실제 간차이(오류)값 나타낸 함수","내용":"MSE : 오차 다 제곱해서 합한 뒤 N으로 나눔(평균)\nRMSE : MSE에 루트\nMAE : 에러의 절대값 총합을 N으로 나눔\nBinary CrossEntropy : 이진 분류기 훈련\n- 예측과 실제 같으면 0에 수렴, 완전 다르면 무한대 출력\n카테고리 CrossEntropy : Class 2개 이상, 원핫 인코딩 라벨\nSparse CrossEntropy : 라벨 원핫 아니고 정수여도 OK"},{"토픽":"딥러닝옵티마이저","키워드":"경 확 모 아 알 담 맥 낵","도식":null,"정의":"비용함수의 최솟값을빠르고 안정적으로 찾기 위한알고리즘","내용":"경사 하강법, 확률적 경사 하강법, 모멘텀\nAdaGrad : 변화 큰 변수는 학습률 작게, 변화 적으면 크게\nRMSProp(Root Mean Square Propagation)\n- 학습 진행될수록 학습률 극단적 감소하는 AdaGrad 개선\nAdam(Adaptive Momentum Estimation) : 모멘텀 + RMS\nAdamax : Adam의 확장 버전\nNAG(Nestrov Accelerated Gradient) : 이동 중지할 지점에\n도달해도 해당 지점 지나치는 문제 해결"},{"토픽":"경사 하강법\n(GD)","키워드":null,"도식":null,"정의":"오류 역전파 시에전체 Data 미분하여기울기 낮은 쪽으로 반복 이동,극값 구해 예측값 도출 기법","내용":"변수 초기값 선정 > 변수 값에 해당하는 경사도 계산\n> 다음 변수로 이동해 기울기 ↓(반복) > 기울기 최소인 변수\nx : 변수 \/ ε : 학습률(옮김의 정도) \/ 분수 : 편미분(함수 경사)\n* 한 번 업뎃마다 전체 Data 미분해야 해서 속도 느림"},{"토픽":"확률적경사 하강법\n(SGD)","키워드":null,"도식":null,"정의":"전체 Data가 아닌랜덤 추출한 일부 Data만 미분,경사 하강법 속도 높인 기법","내용":null},{"토픽":"모멘텀\n(Momentum)","키워드":null,"도식":null,"정의":"확률적 경사 하강법 개선(정확도 개선) 위해 이전 gradient 값과방향을 참고해 변형하는 방법","내용":"모멘텀 : Gradient에 스무딩을 가해 잡음 효과 줄임\n- 질량 * 속도 \/ 신경망에선 질량 = 1 가정, 속도만 표시\nNesterov 모멘텀\n- 다음 이동할 곳을 예견 후 예견한 지점의 기울기를 사용"},{"토픽":"오버슈팅","키워드":"모 네","도식":null,"정의":"학습률 ε가 크게 설정되어학습 속도는 빠르나 너무 큰 값으로 설정해최소로 수렴하지 않는 이상현상","내용":"방지 방안 : 모멘텀, Nesterov 모멘텀"},{"토픽":"데이터분석 기법","키워드":"지 비 준 강","도식":null,"정의":"지도, 비지도, 준지도, 강화 등Data에서 유의미한 결과 찾는분석 기법","내용":"지도 : 회귀 \/ 분류(의사결정, SVM, K-NN, 신경망, 랜포)\n비지도 : 군집(K means), 시각화, 차원축소(PCA), 연관\n준지도 : 적은 label 비지도로 군집분류 > label로 지도학습\n강화 : Agent\/환경\/상태\/행동\/보상 > 최고 보상전략 학습"},{"토픽":"데이터 마이닝","키워드":"선 전 변 마 해\n\n군 의 연 인 사 유","도식":null,"정의":"통계 및 수학적 기술뿐만 아니라 패턴인식 기술들을 이용하여데이터 저장소에 저장된 대용량의 데이터를 조사함으로써의미 있는 새로운 상관관계, 패턴, 추세 등을 발견하는 과정","내용":"통계학 + 기계학습\n(데이터)선택 >(목표데이터)전처리 > (전처리데이터)변환 > (변환데이터)마이닝 > (패턴)해석\/평가 > (지식)\n\n기법 : 군집분석, 의사결정나무, 연관분석, 인공신경망, 사례기반 추론, 유전자 알고리즘"},{"토픽":"오피니언 마이닝","키워드":"자 형 구\n문 의 긍","도식":null,"정의":"SNS의 대량의 리뷰로부터유의미한 정보를 지능적으로유추해내는 Mining 기술","내용":"자연어 처리 : 형태소(명사\/동사\/부사..) + 구문(S V O) 분석\n문장 평가 : 의견 추출(팩트와 분리), 긍\/부정 평가, Scoring\nCrawler > NLP & 텍스트마이닝 > 검색 등 서비스 API"},{"토픽":"지도학습","키워드":"소 흠 회 N K","도식":null,"정의":"데이터 획득 방식 분류 중 라벨 있는 데이터","내용":"SVM, HMM, 회귀분석, NN, KNN"},{"토픽":"KNN\n(K-Nearest Neighbor)","키워드":null,"도식":null,"정의":"분석대상에서 가까운 k개가더 많이 속한 class로대상을 분류하는비모수적 확률밀도 추정 방법","내용":"Tie 없애기 위해 K는 홀수\nK 작으면 오버피팅+노이즈 민감 \/ 너무 크면 언더 피팅\nLazy Learning : 모든 입력값이 들어온 후에야 계산 시작\nFingerprint DB의 Data 등 기반, 위치 측위에 사용"},{"토픽":"의사결정나무","키워드":"규 생 가 타 해","도식":null,"정의":"Data의 속성들로부터분할 기준 속성을 판별,이에 따라 나무형태 모델링하는분류 및 예측 모델","내용":"분리\/정지규칙 > 나무 생성 > 가지치기 > 타당성평가 > 해석\n- 지니, 엔트로피로 purity check 하며 반복적 분할(생성)\n- 패널티 : 잘못 분류할 때, 가지 수 많을 때(오버피팅 방지)\n- 신속, 효과적 시각적 분류\n- 비선형 분류 불가(한 속성을 하나씩밖에 분리 못함)\n타당성 평가 : Validation set으로 test, 이익 도표, 위험 도표"},{"토픽":"의사결정나무평가 기준","키워드":"지 카 일마시파제\n엔 씨 마시파록파","도식":null,"정의":null,"내용":"y가 이산형 : 지니 지수, 엔트로피 지수(불순도 낮을수록 굿)\ny가 연속형 : F통계, 분산 감소량\n지니지수 : CART, 1 - Σ Pi(^2) , 즉 각 종류 확률 제곱합 뺌\n엔트로피 : C4.5, - Σ Pi * log2(Pi), 확률에 로그 해서 뺌"},{"토픽":"의사결정나무종류","키워드":"카 회 분 범 수 지 이\n씨 회 분 범 수 엔 다\n카 분 범 카 다","도식":null,"정의":null,"내용":"CART : 회귀\/분류, 범주\/수치, 지니, 이지분리, 완전모형\nC4.5 : 회귀\/분류, 범주\/수치, 엔트로피, 다지분리, 완전\nCHAID : 분류, 범주, 카이제곱 통계량, 다지분리, 최적모형"},{"토픽":"SVM","키워드":"초 마 커 벡\n\n하 소","도식":null,"정의":"분류를 위한 기준 선을 정의해주어진 Data의 class 판단하는이진 선형 분류 모델","내용":"결정경계: 초평면(Data차원-1) \/ 최적 결정경계 : 최대 마진\n커널기법 : 고차원 공간 통해 비선형 문제 해결\n마진 : 서포트 벡터랑 결정 경계 사이 거리\n마진 작으면(감마↑) 오버피팅, 마진 크면(감마↓) 언더피팅\n\n하드마진 : 오차허용X \/ 소프트마진 : 일부 오차 허용"},{"토픽":"회귀분석","키워드":"모 엪 계 티\nX 단 다중 \/ Y 단 다변","도식":null,"정의":"독립변수가 종속변수에 미치는영향력 측정 및변수 간 관계 설명한 회귀함수추정하는 통계적 방법","내용":"모형은 F 분포 \/ 계수는 t 분포 따름(따라서 각각 F, t 검정)\n선형 : 단순(X가 1개), 다중(X가 2개, 회귀계수 선형결합)\n- Y가 하나면 단변량, 두개면 다변량\n비선형 : CNN, RNN(회귀계수 비 선형결합)"},{"토픽":"로지스틱회귀 분석","키워드":null,"도식":null,"정의":"선형 회귀 분석의 종속 변수를 범주형으로 확장한분류 분석 방법\n\n두개의 값만을 가지는 종속변수와 독립변수의 인과관계, 즉 어떤 사건이 발생할지에 대한 직접 예측이 아닌 그 사건이 발생할 확률을 예측하는 기법","내용":"분석 : 최대우도법(확률변수에서 표집한 값 기반 모수 구함),\n가중 최소제곱법, odds = p \/ (1-p)\n검증 : 혼동 행렬, ROC, Lift 차트, 카이제곱\n활성함수 : Sigmoid, ReLU\n\n결과는 바이너리 (0 or 1) ex> 질병 감염 여부"},{"토픽":"선형 회귀 분석","키워드":"계 최 제\n독 선 등 정","도식":null,"정의":"입력변수와 출력변수 간의수학적 관계에 대한 모형 추정,예측 분석 방법","내용":"회귀계수 : 최소 제곱법(예측-실제 오차 제곱합 최소)\n검증 : F test(모델), T test(계수)\n가정 : 독립성(X 간), 선형성, 등분산성(잔차), 정규성(잔차)"},{"토픽":"선형회귀추론 위배에 따른 모형","키워드":"독 릿 라 엘\n선 다 감\n등 로 퀀\n정 지","도식":null,"정의":"Data가 선형회귀 가정 위배 시 선택하는 적절한 회귀분석 모형","내용":"독립성 : Ridge, Lasso, Elastic net\n선형성 : 다항 회귀(X가 다항식, 곡선), GAM(일반화 기법)\n등분산 : Robust(잔차 절댓값 최소), Quantile(분위값 추정)\n정규성 : GLM(일반 LM, link 함수로 오차 확률분포 지정)"},{"토픽":"다중선형회귀분석","키워드":"다과 정 추 드","도식":null,"정의":"2개 이상의 독립변수와종속변수 간 관계를 설명,종속변수 값 예측에 사용할회귀식을 도출하는 분석방법","내용":"회귀계수 : 최소 제곱법(SSE 최소)\n장점 : Y와 X 복잡 관계를 선형 조합해 단순선형보다 예측 굿\n문제 : 다중공선성(X 간 상관관계랑 p(0.7이상 의심)값 큼) \/ 과적합\n> 정규화(릿지 라쏘), 데이터 추가, 드롭아웃으로 해결"},{"토픽":"다중공선성","키워드":"븨 상 결\n정 축 제","도식":null,"정의":"회귀분석 시독립 변수들 간에강한 선형관계 나타내는 문제","내용":"VIF(Variance Inflation, 분산 팽창 요인) 10 이상일 시\n- VIF = 1 \/ (1 - R^2)\n독립 변수 간 상관계수(피어슨, 스피어만, 켄달) 높을 시\n모델 결정계수(R^2) 값은 큰데 회귀계수의 t 값은 작을 시\n해결 : 정규화(릿지, 랏쏘), 차원 축소(PCA), 변수 제거"},{"토픽":"회귀분석변수 선택법","키워드":"모 에 삐\n단 전 후 단","도식":null,"정의":"Y에 가장 영향을 미치는X들을 최소한으로 선택해최적의 회귀 방정식 도출 방법* 오컴의 면도날(적을수록 굿)","내용":"모든 가능한 독립변수 조합별 회귀모형 분석 : AIC, BIC\n- 최소의 정보 손실(AIC, BIC) 기준 선택\n단계적(SSR 기준) : 전진, 후진, 단계(넣었다가 아니면 빼)\n- 변수별 P value, 모델의 adj R값, F통계량 등 확인"},{"토픽":"모든 조합 분석변수 선택법","키워드":"A 마이록엘 이케\nB 마이록엘 케록엔\n예측은 AIC, 분포 궁금하면 BIC","도식":null,"정의":"모든 변수 조합 회귀모형 기반,AIC, BIC가 가장 낮은 모형을최적 모형으로 선택하는변수 선택 방법","내용":"Akaike Info Criterion(AIC) = -2 log(L) + 2k\nBayesian Info Criterion(BIC) = -2 log(L) + k log(n)\n* BIC가 표본 크기 커질수록 복잡한 모형을 더 강하게 처벌\n* 2log(L) : 모형 적합도 \/ L : 우도 \/ k : 변수 수 \/ n : Data 수"},{"토픽":"변수 선택 기준","키워드":"결 수 에","도식":null,"정의":null,"내용":"R^2(결정계수, 설명력) : 독립변수 적합도, 1에 가까우면 굿\nAdj R^2(수정된 결정계수) : 독립변수 개수 상관 없이 비교\nAIC : 실 분석 시 젤 많이 사용, 표본(n) 다르면 부정확\n* R에서는 step 명령어로 AIC 통한 모든 변수 조합 수행"},{"토픽":"회귀분석계수 축소법\n(정규화)","키워드":"라 릿 엘","도식":null,"정의":"X간 에는 상관성이 적고Y랑은 상관성 큰 독변만 추출,그 외 영향력 없는 변수들의회귀계수 0에 수렴시키는 법","내용":"X1와 X2가 Y의 변동을 설명할 때 겹치면 하나 제거 필요\n> 노이즈 제거로 모형 정확도↑ \/ 연산 속도↑ \/ 해석 능력↑\n다중선형 : SSE 최소화\n계수축소 : [SSE + f(β)]를 최소화\nLasso, Ridge. Elastic net"},{"토픽":"LASSO선형회귀 모델","키워드":"절 원 맨 스","도식":null,"정의":"입력변수들 출력에 미치는 영향서로 많이 다를 때f(β)에 회귀계수의 절대값 합을 대입해 계수 축소하는 방법","내용":"람다가 클수록 계수를 완전히 0으로 만듦\n일부 입력 변수 특성이 상대적으로 더 중요, 과감한 선택 필요\nL1 정규화, 맨하탄 거리, Sparse Model"},{"토픽":"Ridge선형회귀 모델","키워드":"제 투 유 논","도식":null,"정의":"입력변수들 출력에 미치는 영향전반적으로 비슷할 때f(β)에 회귀계수의 제곱의 합을 대입해 계수 축소하는 방법","내용":"람다가 클수록 계수를 0에 가까운 수로 축소\n모든 입력 변수 특성이 비슷하게 중요\nL2 정규화, 유클라디안 거리, Non Sparse Model"},{"토픽":"Elastic-Net선형회귀 모델","키워드":null,"도식":null,"정의":"LASSO와 Ridge의 결합 모델,람다1과 람다2가 각각릿지와 라쏘 속성 강도 조절해변수를 축소하는 기법","내용":"LASSO는 상관관계 있는 여러 변수 중 하나만 랜덤 선택 Elastic Net은 상관성 높은 다수 변수들 전부 선택 or 제거\n> group effect 유도"},{"토픽":"PCA\n(Principal Component Analysis) ","키워드":"공 고 벡 변 선\n랜 점 커","도식":null,"정의":"다차원 공간 데이터에서데이터 분산 최대되는 축 찾고직교하는 축 찾아선형연관 없는 저차원으로 축소(비지도 학습)","내용":"공분산 행렬 > 고유값\/고유벡터 선택 > 변환행렬 > 선형변환\n- 공분산 : 노드 간 관계 \/ 고유값 : 축(선)의 길이\n- 변환 행렬 : 저차원의 새로운 Z 행렬\nX1, X2 선형결합>서로 관련X 변수(직교 축) 생성, 차원축소\n랜덤 PCA(확률적 Algo로 주성분 근사값 빠르게 찾음),\n점진적 PCA(나눠서 분석), 커널 PCA(고차원 매핑)"},{"토픽":"LDA\n(Linear Discriminant Analysis)","키워드":"산 고 벡 저\n\n내 소 클 대","도식":null,"정의":"다차원 공간 데이터에서클래스 내부 분산은 최소,클래스 간 분산 최대되게최적 분류 특성 공간 찾는 분석","내용":"산포행렬(클래스 간, 내부) > 고유값\/벡터 > 저차원 데이터\n클래스가 있으니 지도학습임\n\n데이터를 특정 한 축에 사영(projection)\n두 범주(빨간색, 파란색)을 잘 구분할 수 있는 직선 탐색\n\nLDA는 그림의 LD1을 찾아 독립변수 x를 이용해 분류\/예측 모델"},{"토픽":"SVD\n(Singular Value Decomposition) ","키워드":"유 씨 븨 \/ 상 대 좌","도식":null,"정의":"임의의 M*N 차원 행렬에 대해여전히 직교할 수 있도록행렬을 분해하는 기법(유 씨 븨 \/ 상 대 좌)","내용":"A(m*n 행렬) = U(mm, 상하) * Σ(mn, 대각) * V(nn, 좌우)\nU : 左 특이벡터 \/ Σ : 고유값에 루트 적용 \/ V : 右 특이벡터\n유형 : Full, Thin, Compact, Truncated(풀씬컴트)\nThin : 비대각 제거 \/ 컴팩트 : 0 제거 \/ 트런 : 상위 t개만 씀"},{"토픽":"잠재 의미 분석\n(LSA,Latent SemanticAnalysis)","키워드":null,"도식":null,"정의":"DTM, TF-IDF 가 단어 빈도만따져 의미 고려하지 못하는단점 해결 위해SVD 활용한 분석 기법","내용":"X1 : n개의 문서는 원래 단어 수 m보다 작은 k개 변수로 표현\nX2 : m개 단어는 원래 문서 수 n보다 작은 k개 변수로 표현"},{"토픽":"다변량 분석","키워드":"인 회 분\n상 주 요\n유 척 판 군","도식":null,"정의":"여러 현상이나 사건 측정치를개별적 분석이 아니라동시에 한번에 분석하는,여러 변인 관계 동시분석 기법","내용":"인과관계 분석 : 회귀분석, 분산분석(ANOVA)\n- 차원축소 시 X 선택(전후단), Y별 일변량 분석 후 합침\n- 각 Y 개별 고려 어려우면 다변량 분산분석(MANOVA) 씀\n상관관계로 Data 차원축소 : 주성분분석(PCA), 요인분석\n유사성 기준 개체 분류 : 다차원 척도, 판별분석, 군집분석"},{"토픽":"다차원 척도법\n(MDS)","키워드":"계 비\n유 적 공 최","도식":null,"정의":"다차원의 개체들을2차원 공간에 위치시켜개체 사이 유사성을쉽게 파악하는 분석기법","내용":"계량형 MDS : 등간\/비율, 유클리드, 개체 간 비유사성 표현\n비계량형 MDS : 명목\/서열, 순서 척도를 거리처럼 변환\n개체 간 유사성(거리) 계산 > 추정거리 적합도 표현 >\n공간상에 개체 표현 > 최적모형 도출"},{"토픽":"판별 분석\n(Discriminant Analysis)","키워드":"변 함 정 예\n정 윌 고","도식":null,"정의":"Y가 2개 이상일 때여러개의 독립 변수로집단 구성원 판별\/예측 위한통계적 방법","내용":"선형 판별(LDA) : 클래스별 같은 공분산, 선형 분류\n이차 판별(QDA) : 클래스별 다른 공분산, 비선형 분류\n판별 변수 > 판별 함수 > 분류 정확도 > 소속 집단 예측\n분류 정확도 : Wilk's λ값(집단내 제곱합 \/ 전체 제곱합),\n고유값(집단간 제곱합 \/ 집단내 제곱합), 둘 다 클수록 굿"},{"토픽":"앙상블","키워드":"보 배 부 스","도식":null,"정의":"머신러닝에서하나 이상의 모델 통해 학습한예측\/분류 결과를 종합해의사 결정하는 기법","내용":"높은 신뢰성, 과적합 최소화(이상치 대응↑, 전체 분산 감소)\n훈련집합 도출 > 집합별 모델 학습 > 결과 취합\n유형 : 보팅 배깅 부스팅 스태킹"},{"토픽":"보팅","키워드":"하 소","도식":null,"정의":"각각 다른 알고리즘 이용한여러 학습 모델별로Data set 하나 학습 결과 취합,최종 결정하는 앙상블 기법","내용":"같은 Data로 선형 회귀 \/ 트리 \/ SVM 등 투표 시켜 취합\n- 정확도 높은 모델끼리 결합시 개선, 아닐 시 오히려 나빠짐\n하드보팅 : 다수결 투표\n소프트보팅 : 모델들이 분류별 확률을 예측, 확률을 평균냄\n\n데이터셋참조>모델학습>각 모델 예측>최종예측 도출"},{"토픽":"배깅","키워드":"부 엔 모 어","도식":null,"정의":"Data를 부트스트랩하여각각 예측\/분류모델 생성,각 결과 취합하는병렬적 앙상블 기법","내용":"부트스트랩 > N개 집합 > N개 예측 모델 > Aggregate\n* 부트스트랩 : 랜덤 '복원' 추출\n- 회귀 : 평균 \/ 분류 : 다수 투표\n- 동일 알고리즘으로 여러 모델을 만듦 \/ 분산 줄임"},{"토픽":"랜덤 포레스트","키워드":null,"도식":null,"정의":"다수 의사결정 트리를 앙상블,예측\/분류 위해 생성하는배깅의 한 종류","내용":"N set > N 모델 > 대표 변수 샘플을 leaf로 > leaf 선형결합"},{"토픽":"부스팅","키워드":"부 모 가 결","도식":null,"정의":"Data 전체로 첫 모델 생성 후모델 학습 결과를다음 모델 학습에 전달하는직렬적 앙상블 기법","내용":"1개 Bootstrap Data set > 약한 모델 학습 > 오답에 가중치 >\n다음 모델 학습 > ...(반복) > 약모델들 선형결합(최종 모델)\n- 편향 줄임"},{"토픽":"XgBoost","키워드":null,"도식":null,"정의":null,"내용":"람다(과적합 감소), 학습률이 과적합에 영향"},{"토픽":"스태킹","키워드":"베 예 메 최","도식":null,"정의":"교차 검증 기반으로메타 Data set을 만들어서로 상이한 모델 조합 기법","내용":"Base Learner N개 > 각 예측 > Meta 러너 1개 > 최종예측\n- 부트스트랩 안함, 따라서 같은 Data로 계속 학습해 과적합\n> Data set을 k개로 나눠, 1개는 test, k-1개는 train(k fold)\n> test셋 옮겨가며 k번 반복, 그 결과(meta Data)를 \nmeta train set, meta test set으로 재사용해 학습, 결과 냄"},{"토픽":"비지도 학습","키워드":"클 솜 피 엘 아 케","도식":null,"정의":"데이터 획득 방식 분류 중 라벨 없는 데이터","내용":"Clustering, SOM, PCA, LDA, ICA, K-means"},{"토픽":"군집분석","키워드":"계 유 병 최 평 와\n\n분 군 밀 케 디 조","도식":null,"정의":"Unlabeled Data 기반,유사한 Data끼리 여러 그룹화,성격 파악하는 탐색적 분석기법","내용":"유사성 : 유사도(상관계수, 자카드) \/ 거리(맨하탄, 마할라)\n성능 평가 : 실루엣 계수\n계층 : 유사 Data 군집, 병합군집, 최단\/평균\/와드 연결\n분할 : 군집 수\/밀도, K means, DBSCAN, 자기조직화 지도"},{"토픽":"마할라노비스거리","키워드":null,"도식":null,"정의":"각 변수의 분산과 공분산을함께 고려한 통계적 거리","내용":null},{"토픽":"민코프스키 거리","키워드":null,"도식":null,"정의":"유클리드 거리와 맨해튼 거리의 일반화로 생각할 수 있는 노름된 벡터 공간의 거리","내용":"유클리드, 맨하탄 거리 일반화"},{"토픽":"표준화 거리","키워드":null,"도식":null,"정의":"두 점의 단위 다를 시각 변수의 분산을 고려해표준화한 통계적 거리","내용":"산포 큰 변수의 영향 줄이려고 각 변수의 분산으로 나눠\n표준화한 값들의 유클리드 거리"},{"토픽":"실루엣 계수","키워드":null,"도식":null,"정의":"개별 Data가군집 내 Data와 가까운 정도와타 군집 Data와 떨어진 정도로군집화 결과 평가하는 수치","내용":"a = 나랑, 내 군집에 속한 샘플들 간 평균 거리\nb = 나랑, 내 군집과 젤 가까운 타 군집의 샘플들과 평균 거리\na가 낮고 b가 높으면 군집화 Good"},{"토픽":"병합군집 \/ 계층형 군집화\n(Agglomerative Clustering)","키워드":"초 상 완\n와 평 중 장 단","도식":null,"정의":"초기엔 각 Data를 군집으로,지정 개수 군집만 남을 때까지유사성 제일 높은 두 군집을합쳐 나가는 알고리즘","내용":"계층 군집임(덴드로그램 나옴), 단일 클러스터 되면 군집 끝\n초기 클러스터 정의 > 유사성 계산, 상향 병합 > 군집 완료\n새로운 데이터 포인트에 대해서는 예측 불가\n와드(모든 군집내 오차 제곱합 최소화) \/ 평균 \/ 중심점 \/ 최장 \/ 최단 연결"},{"토픽":"K-Means","키워드":"K 중 완","도식":null,"정의":"n개의 개체를k개의 군집으로 분류하는거리 기반 군집 알고리즘","내용":"Centroid  K개 선정 > 중심 재계산 > 중심 변경 없으면 완료\n- K 선정 : Elbow 기법 \/ 평가 : 실루엣 지표(0.5보다 커야)\n- Elbow : Y축은 Inertia(군집 간 거리 합), X는 군집 수\n- Inertia가 급감하는 지점의 군집 수를 K값으로 선정\n* 중심은 산술평균 기반 측정 \/ lazy learning 알고리즘임\n* DBSCAN으로 노이즈 제거 후 K Means로 고속 군집!"},{"토픽":"DBSCAN","키워드":"밀 코 모","도식":null,"정의":"ε, MinPts, 코어, border 등 밀도 기반의비지도 Clustering 학습 모델","내용":"밀도 기준 : 밀도 반지름 ε, 군집 내 최소 좌표 개수 MinPts\n밀도 기준 선정 > 코어 판별 > 모든 점 방문 시 군집 완료\n기하학적 군집 가능(U, H 모양 등) \/ K Means 보단 느림\n노이즈 제거에 굿(ε 범위 안에 MinPts만큼 없으면 노이즈)"},{"토픽":"생성 모델\n(Generative Model)","키워드":"익 아 정 알 근 배\n임 몰 마 간\n\n트 어 임","도식":null,"정의":"주어진 데이터를 학습하여학습 데이터의 분포를 따르는유사 데이터를 생성하는 모델\n\n대규모 데이터와 패턴을 학습하고 기존의 데이터 활용하여 이용자가 요구하는 새로운 결과를 만들어 내는 인공지능","내용":"Explicit 모델 : Data의 확률 분포 아는 상황에서 학습\n-분포 정확히 정의(Pixel RNN), 분포에 근사(VAE)\nImplicit 모델 : 확률 분포 모른 채로 샘플링으로 추정\n- 마르코프 체인, GAN(분포 안 구하고 경쟁으로 해결)\n\nTractable Density : Pixel RNN \/ Pixel CNN\nApproximate Density : AE \/ VAE\nImplicit Density : DCGAN \/ GAN"},{"토픽":"GAN\n(적대적 생성신경망)","키워드":"생 판 실","도식":null,"정의":"생성자와 판별자 간 경쟁 통해실제와 비슷한 가짜를 만드는비지도 학습 신경망","내용":"생성기 : real Data와 비슷한 모조데이터 생성\n판별기 : real인지 fake인지, 50% 확률로 정답 맞히면 종료\n판별 > 생성 학습 반복, real-fake 간 분포 차이 X면 학습 끝"},{"토픽":"GAN의 한계","키워드":null,"도식":null,"정의":null,"내용":"모드붕괴 : 생성자가 다양성 없이 비슷한거만 계속 생성\nVAE 모델 : 안정적이지만 흐릿한 이미지 생성되는 블러 현상\ntxt 등 불연속 Data 성공울 ↓(이미지는 픽셀 미분으로 개선)\n블랙박스 문제 : 신경망의 근본 문제, 과정의 불명확성"},{"토픽":"DCGAN\n(심층 합성곱 GAN)","키워드":"생 렐 탄\n판 릭 배","도식":null,"정의":"기존 GAN의Fully connected 구조를대부분 합성곱 NN으로 대체,불안정성 극복한 안정화 GAN","내용":"구조 : 합성곱만 두고 fully connected랑 pooling층 없앰\n생성자는 ReLU, Tanh(마지막 결과에서만 Tanh)\n판별자는 Leaky ReLU(모든 layer 적용), 배치 정규화\n한번에 고해상도 이미지 생성 가능, 고성능 GAN의 시초"},{"토픽":"SRGAN\n(Super Resolution GAN)","키워드":null,"도식":null,"정의":"저해상도 이미지를고해상도로 변환하는 GAN","내용":"DCGAN 기반\n회화 그림 관련 응용 프로그램 등에 사용"},{"토픽":"SGAN","키워드":null,"도식":null,"정의":"GAN + 준지도학습","내용":"원랜 Image > 판별기 > Real or Fake 인데,\nReal Data를 '불량'과 '양품'으로 나눠서 label을 해줌\nImage > 판별기 > 불량, 양품, Fake"},{"토픽":"딥페이크","키워드":"GAN, LSTM","도식":null,"정의":"GAN을 통한 생성자와 판별자의 경쟁적 학습으로 개인얼굴을조작해서 합성하는 딥러닝 기술","내용":"랜드마크&실 이미지 -> 생성자 -> 판별자 -> 재학습\n현재와 이전 프레임 차이(residual 프레임)를 CNN과 LSTM\n기반의 RNN을 이용하여 벡터화 처리"},{"토픽":"딥누드","키워드":null,"도식":null,"정의":"얼굴이 아닌 몸을 합성하는딥러닝 기술","내용":null},{"토픽":"자기 조직화 지도(SOM)","키워드":"입 벡 가 \/ 경 격 뉴\n초 경 협 적","도식":null,"정의":"신경망 구조 기반이나역전파 없는 전방 패스로 빠른,N차원 Data를 2차원으로군집화\/차원축소 병행 기법","내용":"입력층 : 입력 벡터(N개, 모든 뉴런과 연결), 가중치 w\n경쟁층 : 2차원 격자(m*m), 뉴런(벡터와 젤 유사한게 위너)\n초기화(w) > 경쟁(Winning Node 선택) > 협력 > 적응\n- 협력 : Winner 반경 내 뉴런들 w 조정(조정 없으면 완료)"},{"토픽":"자기지도학습","키워드":"프(비라) 다(파)\n대 비","도식":null,"정의":"Data 라벨링 어려움 해결,최소 Data로 스스로 규칙 찾아라벨 부여해 학습하는깊이성장 AI","내용":"Pre-trained 모델 생성 > Downstream task\n- Pre : 비지도 학습, 라벨 생성\n- Down : 소량의 라벨로 모델 Fine Tuning\n- 대조적 : 긍정 샘플 거리 최소화, 부정 샘플 거리 최대화\n- 비대조적 : 긍정 샘플만, 샘플을 긍정 분류하는 법만 학습\n* 예시 : BERT"},{"토픽":"준지도 학습","키워드":"분 셀 코\n데 매 하 멀 셀","도식":null,"정의":"소량의 Label Data와대량의 Unlabel 데이터를동시에 훈련하여정확도 높이는 학습 기법","내용":"라벨 학습 > 언라벨 학습 > 조건 만족 시 종료, 아니면 반복\n분류 기반 : Self training, Co-training\nData 기반(패턴) : Manifold 정규화, 하모닉 믹스쳐\n멀티 뷰 러닝(여러 모델), 셀프 앙상블(단일 모델 여러 설정)"},{"토픽":"AutoEncoder","키워드":"인 코 은 디 아","도식":null,"정의":"데이터의 Feature를 추출,Latent Vector 정의,비선형적 차원 축소하는 모델","내용":"Input > 인코더 > 은닉층(레벡) > 디코더 > Output\n- AE는 Latent Vector의 Feature '파악'이 중요(차원축소)\n- PCA는 선형 축소, AE는 비선형 + '점'(단일 값)으로 축소\n- VAE는 Output '생성'이 중요 \/ 분포(Latent Coding)로 축소"},{"토픽":"Manifold","키워드":null,"도식":null,"정의":"대역적으로는독특한 위상수학적 구조이나국소적으로는유클리드 공간 닮은 위상 공간","내용":"예시 : 스위스 롤(국소적으로 보면 평면, 대역적으론 롤)\n- 롤 안에서 밖까지 유클리드로는 가깝, 다 돌아서 나가면 멂\n- 학습 모델의 잠재 벡터는 우리의 예상 차원과 다를 수 있음\n- 매니폴드 학습 시 중간층 차원을 높여(커널) 학습 시 수월"},{"토픽":"VAE\n(Variational AutoEncoder)","키워드":"입 인 레벡 디 아","도식":null,"정의":"가우시안 확률 분포 기반,인풋을 Latent 벡터로 인코딩후스스로 input 복원해Data 분포 배우는 준지도 학습","내용":"입력 > 인코더 > Latent Vector(잠재) > 디코더 > Output\n- 이미 알고 있는 [분포(뮤, 표편)+가우시안 분포] 기반\n- Latent Vector Z : 각 feature의 뮤와 표편\n- Cost = input x와 복원된 x'(디코딩 된 x) 간의 Loss"},{"토픽":"GAN과 VAE 차이","키워드":null,"도식":null,"정의":null,"내용":"GAN : 샘플링 기반 생성\nVAE : 분포 기반 생성"},{"토픽":"능동학습\n(Active Learning)","키워드":"언 중 라 학 추","도식":null,"정의":"labeled Data가 소수인 경우불확실성 큰 Data부터반복적 라벨링하며 학습하는점증적 지도학습","내용":"모델은 불확실성 높은, 즉 라벨링이 효율적인 Data를 찾음\n- 라벨링은 모델이 아닌 사람(Annotator, Oracle)이 함\n- 모델(러너)은 적절한 Query Strategy로 중요 Data 찾음\nUnlabel Data > 중요 Data > 라벨링 > 학습 > Data 추가"},{"토픽":"능동학습시나리오","키워드":"멤 스 풀","도식":null,"정의":null,"내용":"Membership Query Synthesis : 모델이 Data 생성해 쿼리\n- 주어진 이미지 등 분포 기반, 비슷한거 생성 후 라벨 요청\nStream-Base Selective 샘플링 : 신규 Data 들어온거 판단\n- Query Strategy로 정보량 평가, 높으면 쿼리, 아님 버려\nPool Based Sampling : 큰 언라벨 Data Pool 있을 시 사용\n- 전체 Data 중 정보량 높은 Data들 선택. 제일 일반적."},{"토픽":"강화 학습","키워드":"큐 몬 디 말\n에 행 환 상 보 할\n에 폴 밸 모 프","도식":null,"정의":"Agent가 현재 상태에서선택 가능한 행동 중보상을 최대화하는행동을 선택하는 학습* S(t) + A(t) > S(t+1)","내용":"Q-Learning, Monte Carlo, DP, Markov Decision\nAgent(RL Algo) + 행동 + 환경 + 상태 + 보상 + γ(할인율)\n- Agent 구성 : Policy(행동 패턴), Value Function, Model\n- Policy : 결정적, 확률적\n- Value Function : 환경과 행동이 보상 얼마 줄지 예측\n- Model Base : E 앎, S별 어떤 A가 최고 R 주는지 앎\n- Model Free : E 모름, A에 따른 S, R 받아 R 최대화 탐색\n* 모델 Base는 모델 알기가 어렵 \/ Free는 시행착오가 多\n- 할인율 : 빠른 보상에 가중치(울자마자 우유 주면 효과 ↑)"},{"토픽":"Policy Based강화 학습","키워드":null,"도식":null,"정의":null,"내용":"Policy Optimization\n- Policy가 완벽하면 Value Function 필요 없음\n(Value Function은 Policy를 만들기 위한 중간 계산이니까)\n- 원하는걸(Policy) 직접 최적화하니 안정적"},{"토픽":"PPO\n(Proximal Policy Optimization)","키워드":null,"도식":null,"정의":"강화학습 정책 강화 알고리즘","내용":"보상 모델에 PPO 넣어 보상 평가 후 Feedback\n\n저장된 과거의 결과까지 활용하여 결과 생성"},{"토픽":"Q-learning","키워드":"큐 상 행\n큐 행 보","도식":null,"정의":"주어진 상태에서어떤 행동의 보상 기대값을예측하는 Q함수를 학습하는모델 없는 강화학습 알고리즘","내용":"Value Based \/ Q(S, A) \/ 벨만 방정식 기반 Off Policy\n- Value Function이 완벽하면 최고의 선택 가능\n- Q Value = 시간 t에서 전략 π 따라 A 선택 시 미래 R 평균\n- Policy는 자연히 얻어지는 것(Value 높은 방향으로)\n- 주어진 유한 마르코프 결정과정의 최적 정책 찾으려 사용\n- Data를 많이 활용하는 장점\n- Q 초기화 > Q 기반 A 수행 > R 측정 > Q 갱신 > A > R...\n> S'가 최종 Q값, 이걸 모델로 Agent는 최적 정책 따라 행동"},{"토픽":"DQN","키워드":null,"도식":null,"정의":"기존 Q-Learning에신경망을 결합한 모델","내용":"현재 상태 S에서 행동 A를 했을때의 이득값 Q(S, A) 알면\n모든 행동 A에 대해 가장 높은 Q값을 갖는 행동을 수행"},{"토픽":"마르코프결정과정\n(MDP)","키워드":"S P A R 감(스파감)","도식":null,"정의":"과거가 아닌현재의 상태와 행동이미래를 결정하는,Reward 기반 강화학습 과정","내용":"미래를 간결히 수학적으로 표현하기 위한 모델\n> 이 모델 공식을 벨만 방정식으로 표현\n바둑을 그간 어떻게 뒀든, 현재 턴만 알면 다음 행동 가능\n- 운전은 지금 상태만 갖고 다음 행동 결정 불가(MDP 불가)\nS(상태), P(S에서 S'로 이동 확률), A(행동),\nR(S에서의 보상), 감마(γ, 할인율)"},{"토픽":"마르코프 유형","키워드":null,"도식":null,"정의":null,"내용":"마르코프 프로세스(MP) ≡ MP(S, P)\n마르코프 리워드 프로세스(MRP) ≡ MRP(S, P, R, γ)\n마르코프 결정 프로세스(MDP) ≡ MDP(S, P, A, R, γ)"},{"토픽":"벨만 기대 방정식","키워드":null,"도식":null,"정의":null,"내용":"State-Action Value Function : Policy Iteration"},{"토픽":"벨만 최적 방정식","키워드":null,"도식":null,"정의":null,"내용":"State-Only Value Function : Value Iteration\n* 벨만 방정식들은 동적 계획법으로 풀면 효율적"},{"토픽":"은닉 마르코프결정과정","키워드":"아이스크림 > 날씨 추측","도식":null,"정의":null,"내용":"상태가 은닉됨(ex. 날씨)\n아이스크림 1개\/2개\/3개 팔린 날 > 각각 비\/구름\/해\n은닉상태 최적해(마지막 날 날씨는?) : Viterbi 알고리즘"},{"토픽":"연관규칙","키워드":"지 신 향\nApriori로 규칙 도출","도식":null,"정의":"대량 Data로부터,품목 간 연관성 찾아내고정량적 표현하는 작업","내용":"지지도 : P(X ∩ Y)\/P(전체)\n신뢰도 : 지\/P(X),\n향상도 : 신\/P(Y)\n\n탐색적\/비목적성\/사용 편리\/계산 용이,\n계산 많음\/少품목 소외"},{"토픽":"Apriori 알고리즘","키워드":"선 빈 연 조 프\n빈 최 후 연","도식":null,"정의":"연관규칙 도출 시,후보 데이터 집합을 최소 집합으로 범위를 줄이는 알고리즘","내용":"선험 규칙(빈발O\/X), 연산(Join-빈발 생성, Prune-비 제외)\n빈발 탐색 -> 최소지지도 -> 후보집합(반복) -> 연관규칙"},{"토픽":"추천 시스템알고리즘","키워드":"상 연 딥 강","도식":null,"정의":"방대한 Data에서유저가 원하는 Data만선택해 보여주는 필터링 기술","내용":"상관 분석 > 연관 분석 > 딥러닝 기반 > 강화학습 기반\n- 딥러닝 기반 : 협업 필터링, 컨텐츠 기반"},{"토픽":"협업필터링\n(colaborative filtering)","키워드":"유 아\n코 유 피 자","도식":null,"정의":"여러 유저의 Profile 기반으로rating 유사도 높은유저의 item을 추천하는 기술","내용":"유저-유저 : 나랑 비슷한 rating 하는 유저의 item을 추천\nItem-Item : 유사한 item들의 등급으로 해당 item을 평가\n알고리즘 : 코사인 유사도, 유클 거리, 피어슨, 자카드(교\/합)"},{"토픽":"협업필터링 한계","키워드":"콜 효 롱","도식":null,"정의":null,"내용":"콜드 스타트 : 초기 정보 부족해 새로운 항목은 추천 어려움\n– KNN, DBSCAN 등 항목 자체 내용 분석 기반으로 추천\n계산 효율 저하 : 사용자 많으면 행렬 분해로 장기간 계산\n– GPGPU, Grid Computing 등 행렬 최적화 컴퓨팅 사용\n롱테일 문제 : 비대칭적 쏠림현상(관심없는 항목 정보 부족)\n- LDA, 베이지안 NW 등 자료 내 사용자 패턴 기반으로 추천"},{"토픽":"컨텐츠 기반추천 시스템","키워드":null,"도식":null,"정의":"협업 필터링Cold Start 해결 위해한 유저가 매긴 rating 기반유사 item 추천 시스템","내용":"유저 프로필 > Item 프로필 > Item 간 유사도 > 유사 Item 추천\n- 유저 프로필 : 유저 특징, 유저가 각 Item 등급 평가한 것\n- Item 프로필 : Feature(제목, 작가 등), 중요 단어 벡터\n* 한계 : 유사도 Feature 기준 모호, 신규 유저는 추천 어려움\n> 컨텐츠 기반으로 Cold Start 극복 후 협업 필터링"},{"토픽":"SNA\n(Social Network Analysis)","키워드":"범 응 동 명 중\n이 중 군 유 밀","도식":null,"정의":"사회연결망 이론 기반,소셜NW의 형태\/구조\/강도 등측정해 구성원과 NW 영향력 등유의미한 지식을 도출하는 기법","내용":"구조파악 기법 : 범위, 응집력, 구조적 동위성, 명성, 중계\n- 범위 : NW 규모 \/ 응집력 : 행위자 간 강한 사회관계\n- 동위성 : NW 내 지위\/역할 동일한 사람 간 관계\n- 명성 : 누가 권력자인가 \/ 중계 : 타 NW와 연계\n분석 : 이웃(수), 중심도, Clique(결합도로 군집),\nAffinity(유사도), 밀도(실 관계\/총 관계)"},{"토픽":"SNA 분류","키워드":"집 그 행","도식":null,"정의":null,"내용":"집합론적 방법 : 객체 관계를 쌍으로 표현\n- A = (X1, X2), (X2, X1), (X4, X2), (X3, X1) 등\n그래프 이론 : 노드, 엣지, 프로퍼티\n행렬 : 정방(객체를 행, 열에 대칭으로) + 인접행렬(1 or 0)"},{"토픽":"SNA Centrality\n(중심도)","키워드":"디 매 근 위","도식":null,"정의":"한 노드가 그래프 내에서얼마나 중요한지를 나타내는중심성 측정 지표","내용":"Degree : 한 점에 직접적으로 연결된 점들의 수\n매개 : (해당 노드 포함하는 최단 경로 수) \/ (전체 최단 경로)\n근접 : 1 \/ (한 노드에서 다른 모든 노드까지의 최단 거리 합)\n위세 : 해당 노드 주위 노드가 중요하면 얘도 중요하다"},{"토픽":"시계열 분석","키워드":"추 계 순 불","도식":null,"정의":"시간흐름 Data의 정상성 기반,특정 변수를 해당 변수의과거 데이터로 설명,미래에 대한 추세 분석 방법","내용":"정상 시계열 : 정상성 조건 다 만족 > 이 때 시계열 분석 수행\n비정상 시계열 : 정상성 조건 하나라도 불만족\n* 차분(평균 정상화)과 평활화(분산 정상화)로 정상성 만듦\n시계열 성분 : 체계적 성분(추세, 계절, 순환), 불규칙 성분"},{"토픽":"시계열 분석의정상성","키워드":"평 분 공","도식":null,"정의":"시계열의 수준과 분산에체계적인 변동이 없어미래는 확률적으로과거와 동일하다는 성질","내용":"평균이 일정 : 모든 시간 t 에 대해 평균 일정\n분산이 시점에 의존 안함 : 모든 시간 t 에 대해 분산 일정\n공분산은 시차에만 의존, 시점엔 의존 안함\n: X(t-h)와 X(t) 간 공분산과 X(t)와 X(t+h) 간 공분산은 같음"},{"토픽":"시계열 분류","키워드":"자 이 아","도식":null,"정의":"AR, MA 등 정상 시계열과ARIMA 등 비정상 시계열로시계열 자료 분류","내용":"자기상관(AR) : 현 시점 자료는 p 시점 전의 과거자료로 설명\n이동평균(MA) : 현 자료 = 유한개 백색잡음(오차) 선형결합\n자기회귀누적이동평균(ARIMA) : AR + MA \/ 얘는 비정상!\n- ARIMA(p, d, q)에서 d만큼 차분해서 ARMA 만들어야 정상"},{"토픽":"차분","키워드":null,"도식":null,"정의":"현시점 데이터에서d 시점 이전의 Data를 빼정상성을 만족시키는 방법","내용":"1차 차분 : Yt = Xt - Xt-1\n2차 차분 : Yt = Xt - Xt-2"},{"토픽":"자기상관과이동평균 수식","키워드":null,"도식":null,"정의":"ARIMA 수식은 AR - MA","내용":null},{"토픽":"공분산","키워드":null,"도식":null,"정의":"동일한 시간에서2개 확률변수의선형 상관관계가 양인지 음인지나타내는 값","내용":"X 커질 때 Y도 커지는 선형 상관성 > cov(X, Y) 가 양수\n공분산은 두 변수 간 상관관계가 양인지 음인지 알려줌,\nbut 상관관계가 얼마나 큰지는 모름 > 상관계수로 알아내야"},{"토픽":"자기 공분산\n(Autocovariance)","키워드":null,"도식":null,"정의":"서로 다른 2개의 시간에서특정 확률변수의 시간 따른선형 상관관계가 양인지 음인지나타내는 값","내용":"자기 자신의 과거, 현재, 미래 간 공분산(자신과의 공분산)\nX가 정상성 시계열 변수면 X(t-h)와 X(t) 간의 공분산과\nX(t)와 X(t+h) 간의 공분산은 일정"},{"토픽":"상관계수","키워드":null,"도식":null,"정의":"특정 동일 시점에서두 변수 간 선형 관계의 정도를수량화하는 측도","내용":"상관계수 r = -1 ~ 1 \/ 절댓값이 1에 가까울수록 연관성 ↑\nr = 공분산 \/ 표준편차 * 표준편차"},{"토픽":"자기 상관계수\n(Autocorrelation)","키워드":null,"도식":null,"정의":"서로 다른 시점의동일 변수 값의선형관계 정도를 수량화한 측도","내용":"자기 자신의 과거, 현재, 미래 등 시점 간 상관관계\n- 이게 높으면 정상성이 없음\n- 시차가 커지면 상관성 떨어져야 하는데 계속 높으니까\n- 이전 시점과의 시차를 Time Lag 라고 함"},{"토픽":"ACF\n(Autocorrelation Function)","키워드":null,"도식":null,"정의":"시차에 따른일련의 자기상관을 나타낸,MA의 차분값 도출 위한 함수","내용":"시차가 커질수록 ACF는 0에 가깝(시차 커질수록 상관 ↓)\n- 정상 시계열은 ACF가 상대적으로 빨리 0에 접근\n- 비정상은 ACF가 천천히 감소, 종종 아주 큰 양의 값\n- Y축 : 자기상관 계수, X축 : 시점"},{"토픽":"PACF\n(Partial Autocorrelation Function)","키워드":null,"도식":null,"정의":"시차가 다른두 시계열 데이터 간의상호 연관성 나타낸,AR의 차분값 도출 위한 함수","내용":"순수하게 특정 시점 간의 연관성만 확인\n- t 시점과 t-1 간 연관성, t와 t-2 간 연관성 등\n- 튀는 시점 1개면 AR(1) 써야겠다 이렇게 알 수 있음\n- Y축 : 편자기상관 계수, X축 : 시점"},{"토픽":"시계열 DB\n(TSDB)","키워드":"디 파 청\n매 포 키 택 필","도식":null,"정의":"time-stamp를 기반으로 하는 저장소로 이를 통해 데이터를 압축하고 요약하는 등의 작업을 진행하여 대규모의 시간 기반 데이터들을 다룰 수 있고, 시간을 기반으로 하는 쿼리를 가능케 하는 데이터베이스","내용":"주기마다 Data 처리\/삭제\nDB > Partition(2000년, 2001년…) > Chunks(RDB의 샤드)\nRDB : Table, Row, Col, Index Col, Unindexed Col(일반)\nTSDB : Measurement, Point, Key, Tag Key, Field Key"},{"토픽":"객체 검출","키워드":"클 로 바","도식":null,"정의":null,"내용":"사물 분류(Classification) + 그 사물의 위치(Localization)\n- 위치는 Bounding Box Regression으로 나타냄\nCNN은 한 물체를 분류, 위치 찾은 후 모든 영역 슬라이딩\n> 간단하나 여러 번 실행해 느림, 많은 객체 탐지 어려움"},{"토픽":"IoU\n(Intersectionover Union)","키워드":null,"도식":null,"정의":"머신러닝의 객체 검출 시2개의 영역이\"얼마나 겹쳐져 있는가\"를기반으로 검출 성능 표시 지표","내용":"정답 영역과 예측 영역이 겹쳐진 부분 클수록 IoU 커짐\n- IoU 클수록 물체가 잘 검출 되는 것, 값은 0~1"},{"토픽":"CNN\n(Convolutional Neural Network)","키워드":"입 콘 풀 풀커 소렐","도식":null,"정의":"합성곱으로 특징 추출 및풀링으로 통합을 계속 반복해 고차원 특징을 추출, 분류하는인공신경망","내용":"입력 > Convolution > Pooling > Fully Connected(분류)\n> MLP : Softmax, ReLU 통해 Output 계산(차냐 트럭이냐..)\n성능 개선 : Convolution + ReLU, Drop Out(과적합 해결)"},{"토픽":"CNN 구성 요소","키워드":"인 합 풀 엠 아","도식":null,"정의":null,"내용":"Input : 컬러이미지는 2차원 배열 각각 RGB 있는, 3차원 배열\n합성곱 : 특징필터를 이미지 처음부터 stride만큼씩 움직이며\n값을 곱해줌 > Feature Map 나옴 \/ 필요시 패딩 씀\nPooling(Max, 평균) : 차원 축소, 통합 \/ 주로 Max 씀\nMLP : 풀콘, 딥러닝 기본 신경망(인풋, 히든, 아웃풋)\nOutput : MLP 단계와 동일, 활성 함수로 Softmax, ReLU 씀"},{"토픽":"CNN 패딩","키워드":null,"도식":null,"정의":"합성곱 연산 전,Output 공간 크기 조절 위해입력값 주변을 0, 1 등특정값으로 채워 늘리는 것","내용":"채울 값 : 하이퍼 파라미터임, 개발자가 결정(주로 zero 패딩)\n패딩 사용 않을 시, 데이터의 Spatial 크기가 Conv Layer를\n지날 때 마다 작아져 가장자리 정보들이 사라지므로 패딩 씀"},{"토픽":"CNN MLP\n(Multi Layer Perceptron)","키워드":null,"도식":null,"정의":"CNN Fully Connected Layer에쓰는 다층 퍼셉트론","내용":"이전 Pooling 단계에서 발생된 데이터를 1차원으로 옮겨\nMLP에 입력\n은닉층 몇 개 할건진 개발자의 선택"},{"토픽":"CNN과 RNN 차이","키워드":null,"도식":null,"정의":null,"내용":"CNN : 정해진 크기의 인풋 받아 정해진 크기의 데이터 출력\n- Feature 추출, 풀링(sub 샘플링해 차원 축소), 분류(풀콘)\nRNN : 랜덤한 입력값을 받아 랜덤한 Data 출력\n- 방향 cycle, 반복 가중치, 시간 역전파(BPTT), 순차 Data"},{"토픽":"RNN\n(Recurrent Neural Network)","키워드":"병렬 불가라 느림","도식":null,"정의":"연속된 데이터 상에서이전 순서 셀 값 저장해다음 순서 학습할 때이전 저장값 사용하는 모델","내용":"X(t) = t 시점에 들어온 입력값 \/ W(in) = 인코딩 위한 벡터 \/\nW(out) = 디코딩 위한 벡터 \/ Z(t) = 인코딩된 결과 \/\nY(t) = t 시점에서의 디코딩 결과 값\n성능 개선 : LSTM, GRU"},{"토픽":"Convolutional LSTM","키워드":null,"도식":null,"정의":"CNN으로 이미지 특징 벡터를먼저 추출하고,이후 RNN(LSTM) 쓰는 방법","내용":"RNN에 2차원 이미지 입력 시 차원 급증하는거 방지\nRNN에서 시간적 의미뿐 아니라 공간적 의미까지 처리 가능\n기상, 날씨 예측이나 동영상 관련 문제에 활발히 연구됨"},{"토픽":"LSTM\n(Long Short Term Memory)","키워드":"포 인 아","도식":null,"정의":"RNN 장기 의존성과기울기 소실 해결 위해Forget, Input, Output의3개 Gate 사용하는 알고리즘","내용":"Forget : 정보 얼마나 잊을지(장기 상태(c)의 어디 삭제할지)\nInput : 현재 상태의 input(x)를 얼마나 기억할 것인지\nOutput : 계산된 값중에 얼마나 다음단계로 보낼것인지\nc(t-1)이 forget에서 (-), input에서 (+), Out에서 tanh, 출력\n재귀 W를 반복해서 곱하지 않고 더해서 장기 의존성 해결"},{"토픽":"GRU\n(Gated Recurrent Unit)","키워드":"리 업","도식":null,"정의":"RNN의 장기 의존성 해결 위해Reset, Update의2개 Gate 통해Data 정보 업뎃하는 알고리즘","내용":"Reset : 이전 hidden state에서 얼마나 값 반영할지\n- 이전 hidden state와 입력을 받아 sigmoid 처리\nUpdate : 이전 정보 조금 + 새로운 state 계산해서 결정함\n- Forget + Input 한번에 처리"},{"토픽":"R-CNN\n(Region-based CNN)","키워드":null,"도식":null,"정의":"CNN 다중객체 인식 한계 극복, Bounding Box 기반사물의 영역 탐지\/특징 추출해분류하는 알고리즘","내용":"영역 탐지 : Selective Search, Region Proposal\nCNN 특징 추출 : Warping, CNN\n영역 분류 : SVM, 바운딩 박스 회귀(실 영역-예측 영역 오차)\n> 2000 개 Box마다 CNN, SVM, 회귀의 3개 모델 학습 > 느림"},{"토픽":"R-CNN 기법","키워드":"전 영 씨","도식":null,"정의":null,"내용":"전체 pixel > 영역 추출(Warped Image) > CNN 수행\n- Selective Search, Region Proposal로 영역 추출함\n- 영역 추출 후 ROI 분석해 CNN 수행\n- CNN : Feature Map > Pooling > Label"},{"토픽":"Faster RCNN","키워드":"인 피 알 풀 분 바","도식":null,"정의":null,"내용":"Input > Feature Map > ROI 분석(Region Proposal)\n> Feature Map과 ROI 더해 Pooling > 분류 + 바운딩 박스"},{"토픽":"YOLO\n(You Only Look Once)","키워드":"그 바 클 결","도식":null,"정의":"classification, localization을1 stage에 해결하는bounding box 기반객체 검출 기법","내용":"S*S Grid > 바운딩 박스들 & Class Map > 결과"},{"토픽":"GNN\n(Graph Neural NW)","키워드":"변 취 결 생 출","도식":null,"정의":"노드 및 엣지로 구성된그래프 데이터에 적합한신경망 알고리즘","내용":"목표 : 이웃 노드 특성\/인접 정보로 어떤 노드의 특징 표현\nData 변환 > GNN Layers > 출력(노드\/엣지\/Graph 임베딩)\n- 변환 : Data > 인접\/특성 행렬 등 그래프로 표현되는 구조\n- 층 : 취합(Aggregate) > 결합(Combine) > 생성(Readout)\n* 각 층별로 취 결 생 반복하는 것"},{"토픽":"NLP","키워드":"희 밀","도식":null,"정의":"사람이 쓰는 자연어를기계가 이해할 수 있는 벡터로바꾸는 기술","내용":"희소 표현 : 0과 1, 단어 수가 벡터 길이(공간↑), 원핫 인코딩\n- I love you > I : 0, love : 1, you : 2 > love = [0, 1, 0]\n밀집 표현 : 벡터 길이(차원) 고정, 모든 단어를 실수로 표현\n- love = [0.2, 1.8, -1.1 ..] 차원 5로 설정하면 길이 5 고정"},{"토픽":"One hot Encoding","키워드":null,"도식":null,"정의":"단어 집합 생성 후인덱스에 고유한 정수를 부여,해당 Index에 1, 나머지는 0을 넣어 원-핫 벡터 생성하는 기법","내용":"단어 집합 : 텍스트의 모든 단어 중복 없이 모은 것\n- Book 과 Books 도 다른 단어로 인식\n벡터\/행렬의 대부분이 0으로 표현되므로 희소 표현\n단어의 유사도를 표현 불가 > 카운트 기반(LSA, HAL)과\n예측 기반(NNLM, RNNLM, Word2Vec, FastText)로 극복"},{"토픽":"Word Embedding","키워드":null,"도식":null,"정의":"단어를 밀집 벡터로바꾸는 기술","내용":"원핫 인코딩 결과를 임베딩 하여 밀집 벡터로 생성\n> 밀집 벡터를 임베딩 벡터라고도 부름"},{"토픽":"NER\n(Named Entity Recognition)","키워드":"임 엘 클","도식":null,"정의":"문장 내에서사람, 장소, 시간 등미리 정해둔 '이름 있는 개체'를인식\/추출하는 분류 기법","내용":"Embedding(벡터화) > BiLSTM(의미 이해) > CRF Classfier\n- BiLSTM : 양방향 LSTM(문장 앞\/뒤로부터 각각 읽어 이해)\n- CRF Classifier : 벡터를 다시 한국어로 바꾸는 분류기\n서현이는 2017년 입사했다 > 서현 : 사람, 2017년 : 시간"},{"토픽":"단어 기반자연어 처리","키워드":"카 보 디 티\n예 워 패\n혼 글","도식":null,"정의":null,"내용":"카운트 기반 : BoW, DTM, TF-IDF\n예측 기반 : Word2Vec, FastText\n혼합 기법 : GloVe"},{"토픽":"BoW","키워드":"빈 벡","도식":null,"정의":"하나 문서 대상으로순서 고려 없이 단어별 빈도를벡터로 만드는 기법","내용":"분류 문제, 문서 간 유사도 구할 수 있음\n- ex) 달리기, 체력 자주 등장 > 체육 문서\n예시 : I love you. You love me.\n- Vocabulary : {I : 0, love : 1, you : 2, love : 3, me : 4}\n- BoW 벡터 : {1, 1, 2, 1, 1} * you만 두 번 등장"},{"토픽":"DTM","키워드":"빈 행","도식":null,"정의":"BoW를 결합해여러 문서의 단어 빈도를행렬로 만드는 기법","내용":"BoW나 DTM은 불용어(stopword) 정제로 정확도 향상\n- the, a 등 빈도 높다고 중요하지도, 유사하지도 않음\n> TF-IDF로 중요 단어에 가중치"},{"토픽":"TF-IDF\n(Term Frequency-Inverse Doc Frequency)","키워드":"단 문 역","도식":null,"정의":"DTM에 가중치를 더해전체 문서에서의 빈출 단어는가중치가 낮고,특정 문서에서만 빈출 시중요도가 높다고 판단하는 기법","내용":"TF(d, t) : 문서 d에서의 특정 단어 t 횟수 \/ d의 전체 단어 수\nDF(t) : 특정 단어 t가 등장한 문서의 횟수\nIDF(D, t) : DF의 역수인데, log(D \/ (1+DF(t))로 역수 취함\n* D : 문서의 총 개수 \/ TF-IDF = TF * IDF\n* log를 취해야 희귀 단어 가중치를 현실적으로 줄임\n* 주로 자연로그(밑이 자연상수 e)인 ln을 사용"},{"토픽":"Word2Vec","키워드":"유 벡\n씨 스","도식":null,"정의":"카운트 기반 기법들의의미 유사도 측정불가 극복,단어 간 유사도를 벡터화 한 예측 기반 임베딩","내용":"단어를 벡터 평면에, 유사 단어는 가까이 배치(문맥 보존)\nCBOW, Skip Gram으로 구성\n- 전체 문서 고려 어려워 GloVe 등장"},{"토픽":"CBOW","키워드":"원 타 예","도식":null,"정의":"원본 Context 단어들로부터타겟 단어를 예측하는 기법","내용":"주변 단어들(Input) > Projection > 중간 단어 예측(Output)"},{"토픽":"Skip Gram","키워드":"타 원 예","도식":null,"정의":"타겟 단어로부터원본 단어들 역으로 예측 기법","내용":"한 단어(Input) > Projection > 주변 단어 예측(Output)"},{"토픽":"FastText","키워드":null,"도식":null,"정의":"단어를 글자로 분할해오타 정정, 신규 단어 유추하는예측 기반 임베딩","내용":"단어를 글자로 쪼개 오타(OOV 문제) 잡음, 신규 단어 유추\n고채 액체 기체 이런거 유사도 보는 애??"},{"토픽":"GloVe","키워드":null,"도식":null,"정의":"Word2Vec의 개선으로단어 간 유사도 및전체 문서 통계 동시 고려하는예측 기반 임베딩","내용":null},{"토픽":"문장 기반자연어 처리","키워드":"알 셐 어 트","도식":null,"정의":null,"내용":"RNN > Seq2Seq > Attention > Transformer\n- 문장 특성 상 이전 문장이 문맥 영향\n- 따라서 이전 Data 활용하는 RNN이 기반이 됨"},{"토픽":"Seq2Seq","키워드":"인 컨벡 디","도식":null,"정의":"한 문장(시계열 Data)을다른 문장으로 변환하는Context Vector 기반 모델(인 컨벡 디)","내용":"Encoder > Context Vector(벡터 한 개) > Decoder\nEncoder, Decoder은 각각 여러 LSTM으로 구성\nDecoder은 SOS로 시작해서 EOS 들어가면 끝난다\n근데 문장 길어지니 힘들어"},{"토픽":"Attention","키워드":"쿼 키 밸","도식":null,"정의":"Seq2Seq의 CV 한계 극복 위해RNN 기반으로중요 단어만 분석하는문장 임베딩(쿼 키 밸)\n\nQ에 맞게 K를 이용해 V에 가중치를 주는 기법","내용":"Seq2Seq은 CV로 한정되는 한계(차원 축소 등 특성 누락)\n> attention으로 의미가 있는 부분만 보겠다 \nQuery : t 시점의 디코더 셀에서의 은닉 상태\nKeys : 모든 시점의 인코더 셀의 은닉 상태들\nValues : 모든 시점의 인코더 셀의 은닉 상태들\n주어진 Q에 대해 모든 K와 유사도 구함\n> 이 유사도를 키와 매핑된 각각의 V에 반영\n> V를 모두 더해 Attention Value로 리턴"},{"토픽":"Attention 매커니즘","키워드":"히 스 컨 ","도식":null,"정의":null,"내용":"① Encoder의 모든 Hidden States 전달\n② Attention Score 계산\n- Decoder의 첫번째 Hidden State와 Encoder의 Hidden States들 각각 내적\n→ Softmax Score로 변환\n③ Context Vector 계산\n- Encoder의 Hidden States에 각 Score 곱한 후 SUM\n④ Context Vector와 디코더의 Hidden State를 Concatenate\n⑤ Concatenate 된 벡터를 FeedForward Neural Network에 입력\n⑥ 해당 스텝의 단어 Output 출력\n"},{"토픽":"Self Attention\naka multihead attention","키워드":null,"도식":null,"정의":null,"내용":"Encoder 여러 개 안에서 서로 스스로 참조\nQ, K, V가 다 '입력 문장의 모든 단어 벡터들'로 동일\n앞 문장에서 표현한 단어를 뒷 문장에서 This로 표현하면\n입력 문장 내 단어끼리 유사도 구해주므로 연관성 찾기 쉬움"},{"토픽":"Transformer","키워드":"문 인 디 아","도식":null,"정의":"Attention is All U Need","내용":"인풋 문장 > 인코더 1, 2.. 6 > 디코더(6개) > Output 문장\n- 인코더 6번이 디코더 1~6에 각각 다 뿌려주는 그림\nRNN 버려 빠르나 순서정보 사라짐 > Positional 임베딩 씀\nSelf Attention 인코더 디코더 Feed Forwarding"},{"토픽":"Transformer Encoder","키워드":"포 셀 피","도식":null,"정의":null,"내용":"Positional 인코딩 > 셀프 어텐션 > Feed Forward\n- 성능 높이려 최소 6개 인코더 병행 : Multi Head Attention\n> 이걸 디코더에서 쓰면 : Masked Multi Head Attention"},{"토픽":"Transformer Decoder","키워드":"포 마 인 피","도식":null,"정의":null,"내용":"Positional 인코딩 > Masked Self Attention > 인코더\/디코더 어텐션 > Feed Forward\n- Look Ahead : 더 미래에 있는 단어 참고 못하게 마스킹"},{"토픽":"BERT\n(양방향 인코더Represent from Transformer)","키워드":"구글","도식":null,"정의":"구글이 개발한,Transformer Encoder 이용한양방향성 Pre Train 가능한문장 의미 추출 임베딩 모델","내용":"기술 : Fine Tunning, MLM(Masked Language Model), next sentence prediction\n\n- MLM : 특정 단어들을 Masking 하고 그거 유추하는 학습\nInput 2 > 은닉 1 > Output 2 이런식으로 지그재그 가능\n어제 > [거기] ← 갔었다(양방향으로 '거기' 추론)\n\n- next sentence prediction : 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 것 \/ pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게"},{"토픽":"GPT-1","키워드":null,"도식":null,"정의":"OpenAI가 만든 언어예측 모델,생성하는(Generative)사전 학습된(Pre-trained)트랜스포머(Transformer)","내용":null},{"토픽":"GPT-3","키워드":null,"도식":null,"정의":"Open AI가 개발한,Transformer Decoder 기반단방향성 Pre Train 가능한문장 생성 임베딩 모델","내용":"Few Shot Learning\nInput 1 > 은닉 2 > Output 2 이렇게 단방향으로만 가능\n어제 거기 갔었다>(단방향으로 문장 생성)"},{"토픽":"ChatGPT","키워드":"예 강 환","도식":null,"정의":"GPT-3.5 기반으로강화학습을 사용하여대화에 최적화된 AI 챗봇 (예 알 환)","내용":"구성 : 보상 예측기, 강화학습 알고리즘, 환경\n- 보상 예측기가 예측한 보상을 알고리즘에 넣음\n- 알고리즘이 환경에 Action, 환경은 알고리즘\/예측기 관찰\n- 결과 보고 사람이 보상 예측기에 Feedback\n- Fine Tuning, Pre Training, RLHF, 파라미터 1750개\n- RLHF : Human의 Feedback Data 기반 강화학습"},{"토픽":"ChatGPT학습 절차","키워드":"프 보 강 피","도식":null,"정의":null,"내용":"Pre Trained 모델에 프롬프트\/기대답변 Data 지도 학습\n> 보상 학습(답변 순위 부여, 답변 순위 예측)\n> 강화학습으로 Fine Tuning : PPO, 신규 Data 투입\n- PPO : Proximal Policy Optimization"},{"토픽":"Instruct GPT","키워드":null,"도식":null,"정의":"GPT-3 기반으로사람이 Feedback한 정보로정확한 정보 생성하는 모델","내용":"GPT 3 > Reward 모델 > 평가 > PPO > GPT 3 Feedback\n- PPO : 예측된 보상으로 policy 업데이트"},{"토픽":"PromptEngineering\n(프롬프트 엔지니어링)","키워드":null,"도식":null,"정의":"생성적 AI의결과물 품질 향상 위한프롬프트의 조합 찾는 작업","내용":"Hallucination(환각) 현상 해결"},{"토픽":"튜링 테스트\n(Imitation Game)","키워드":"대 캡 화","도식":null,"정의":"기계가 인간과 얼마나 비슷하게 대화할 수 있는지를 기준으로기계에 지능이 있는지판별하는 테스트","내용":"질의자 1명, 응답자 둘(컴퓨터 1, 인간 1)\n- 응답은 키보드로만 \/ 질의자가 컴퓨터 판별 불가 시 통과\n- 컴이 인간처럼 대화가능하면 인간처럼 사고하는거라고 봄\n단계 : 대화 > CAPTCHA(변형 글자, 사진 분류) > 화상 전화"},{"토픽":"CAPTCHA","키워드":null,"도식":null,"정의":"웹사이트에 접근하는 주체가사람인지 봇인지 판단 위해사용되는 튜링 테스트","내용":"글자, 사진 등에 비가역적 연산으로 변형을 준 후 맞히게 함\n- 비가역 : 역연산이 존재하지 않는 연산(복원 불가)\n단점 : 시각 장애인 Text Reader는 이거 못읽어 웹접근성↓"},{"토픽":"중국어 방\n(Chinese Room)","키워드":null,"도식":null,"정의":"튜링테스트로는기계의 인공지능 여부를판정할 수 없음을 논증 위해존 설이 고안한 사고실험","내용":"중국어 모르는 사람에 중국어가 매치된 프로그램 책 제공\n- 심사관이 중국어로 질문 써서 방에 넣어주면\n- 참가자는 중국어 아예 몰라도 책 보고 중어로 답 제출 가능\n- 즉 중어를 진짜 '이해'한건 아니고, 단순 '모방'이다"},{"토픽":"BART\n(Bidirectional Auto-Regressive Transformer)","키워드":"메타(페북)","도식":null,"정의":"META(Facebook)가 개발한,BERT와 GPT를 결합한Denoising Autoencoder","내용":"BERT로 받아 GPT3로 쏜다"},{"토픽":"T5","키워드":null,"도식":null,"정의":"구글이 개발한,트랜스포머 인코더\/디코더 기반문장 내 단어 추론 임베딩\n\n외부 데이터에 접근하지 않고 사전학습 시에 입력된 데이터만을 바탕으로 질문에 대답하는 자신만의 ‘지식’을 가진 자연어처리 모델","내용":"Fine Tunning\n\nBERT : 클래스 레이블이나 입력 범위와 같은 인간이 언어 그대로 이해할 수 없는 데이터만 출력\nT5 : 입출력 항상 텍스트 형식이 되도록 자연어처리 작업 재구성\n\n'C4' 데이터 세트 : 웹 스크레핑에 의해 준비된 데이터 세트 \/ 중복,불완전,,과격,노이즈를 제거한 110억의 파라미터 \/ 고품질, 다양성 확보\n\n핵심기능 \n‘알 수 없는 질문에 대한 답변’ \/  자신만의 지식 보유 자연어처리모델"},{"토픽":"멀티 모달 AI","키워드":"일 이 음 데","도식":null,"정의":"텍스트, 이미지, 음성 등 다양한 모달리티를 동시에 받아들이고 사고하는 AI 모델","내용":"일반언어 : NPL, NLU, NLG, 워드임베딩\n이미지 : 이미지 스케일링, 필터링, Morpology\n음성 : STT, Signal Process\n데이터마이닝 : 회귀분석, 시계열분석, 클러스터링, 연관분석"},{"토픽":"메타러닝","키워드":null,"도식":null,"정의":"Few Shot Learning 위해데이터 특성에 맞춰 모델 구조 변화시키며 학습하는,배우는 법을 배우는 기법","내용":"사람처럼, 적은 Data만으로도 샌들\/부츠를 구분하게 함\n- Few shot learning"},{"토픽":"Few Shot Learning","키워드":"제 원 퓨","도식":null,"정의":"적은 Data를라벨 없이도 효율적 학습하는Support, Query 셋 기반 학습","내용":"Support Set : 학습에 사용하는 Data\nQuery Set : 테스트에 사용하는 Data\nN-way K-shot : N 종류 각각 K개\n- 성능은 N에 반비례, K에 비례\n제로,원,퓨샷\n\nData-Driven Approach : Support Set Transformation \/ GAN 이용 충분한 양의 데이터 생성\n\nModel-based Approach : 모델이 동일\/상이 이미지를 구분할 수 있게Feature vector간 Similarity 학습 또는  Overfitting되지 않도 Regularization"},{"토픽":"메타 러닝과 퓨 샷 러닝 기법","키워드":"거 샴 모 만 최 믈","도식":null,"정의":null,"내용":"거리 기반 : 같은 Class면 거리 가깝게, 다르면 멀게 해 분류\n- 쿼리 Data를 거리 가까운 Support Set의 클래스로 예측\n- 샴 네트워크가 대표적\n모델 기반 : 모델에 별도 외부 메모리 둬서 학습 속도 조절\n- 新 정보 신속 인코딩 \/ MANN(Mem Augment Neural NW)\n최적화 : 적은 샘플로 기울기 최적화하는 방법\n- MAML : 각 Data로 전체 기울기 업뎃 > Data별 세부 업뎃"},{"토픽":"전이학습\n(Transfer Learning)","키워드":"프 클 파","도식":null,"정의":"거대한 Data Set 구축 어려워타 도메인의 큰 Data로 학습한가중치 일부를신규 Data에 맞춰 학습 방법","내용":"Pre-Trained 모델 선택 > Classifier 도입 > 파인튜닝\n- Pre Trained 모델 : VCG, GoogLeNet, ResNet\n- Data 少 : ImageNet으로 학습한 CNN 구조 + 뒤에 FC\n- ImageNet과 유사한 Data 多 : 뒷단에 여러 FC층 결합\n* FC : Fully Connected, 분류 위한 Layer\n기법 : Fine-tuning, Multi-Task Learning"},{"토픽":"파인튜닝 기법\n(Fine tuning)","키워드":"전 컨클 컨클 클","도식":null,"정의":"ImageNet 등의 큰 Data로Pre Train된 백본 기반으로Feature Map 도출 뒤FC Layer만 재설계하는 기법","내용":"우리 Data set의 양 + 다른 도메인 Data와의 유사도로 구분\n양↑, 유사↓ : 전체 모델 재학습(어차피 우리 Data 양 많으니)\n양↑, 유사↑ : Convolution층 뒷쪽과 FC(Classifier) 재학습\n양↓, 유사↓ : Convolution층 일부와 Classifier 재학습\n양↓, 유사↑ : FC만 재학습(과적합 방지 위해 기존 피쳐 활용)"},{"토픽":"연합학습\n(Federated Learning)","키워드":"연 엣 에","도식":null,"정의":"개별 device나 분산 서버에서스스로 Data 처리 후학습 알고리즘과 결과 Data만중앙 cloud로 공유하는 학습","내용":"로컬 Data \/ 모델 \/ 연산(학습), Aggregation, 암호통신\nlocal model 연산 > global model로 취합\nFedSGD : 각 단말이 Epoch 1회 학습 후 global 취합 \nFedAVG : 각 단말이 N개의 Batch를 각 학습 후 global 취합"},{"토픽":"연합학습 보안문제","키워드":null,"도식":null,"정의":null,"내용":"보안문제\n단말 > 서버 전달 Gradient 값으로 샘플의 batch위치 확인\nGAN통해 유사 학습 데이터를 생성 \/ 원본 데이터 복원\n\n해결책(학습방법 \/ 추가 알고리즘 적용)\n프라이버시 보장형 연합학습(Privacy-Preserving FL),보안 연합학습(Secure FL), 보안 및 프라이버시 보장형 연합학습(Secure and\nPrivacy-Preserving FL)\n차등정보보호(Differential Privacy), 동형암호(Homomorphic Encryption), 안전한 다자간 계산(Secure Multi-Party Computation)"},{"토픽":"연합학습 유형","키워드":"디 사 모","도식":null,"정의":null,"내용":"Cross Device FL : 불특정 다수 참여(WAN)\n- 특징(희귀병 등) 가진 Data는 다 수집해서 분석\n\nCross Silo FL : 특정 기업 등만 참여(WAN)\n- 동일 ID 특징 분석, 추천 등\n\nCross Domain FL : 같은 기업, 기관내 서로 다른 부문, 부서(LAN)"},{"토픽":"메타 휴리스틱스","키워드":"표 목 초 파 종\n유 시 터","도식":null,"정의":"특정 문제에 한정되지 않고어떤 문제에든 범용 적용되는전역최적화를 위한 알고리즘 틀","내용":"공통요소 : 표현방법(행렬, 그래프 등), 목적함수(좋은 해로\n탐색 유도), 초기해, 파라미터 조정, 종료 조건\n유전 알고리즘, Simulated 어닐링(담금질로 local min 회피),\n터부탐색(이동 가능한 모든 이웃해 중 터부만 제거하고 탐색)"},{"토픽":"유전자 알고리즘","키워드":"부 적 자 최\n선 토 룰 랭\n교 일 산 다 균\n돌 정 동","도식":null,"정의":"NP Hard의 최적해 찾기 위해적자생존과 진화 원리에 기반,부모-자식 집단 생성을 반복해 최적의 해를 찾는 알고리즘","내용":"부모 집단 > 적합도(NO?) > 자식 집단(반복) > 최적해\n자식 생성 : 선택, 교배, 돌연변이\n선택 : 토너먼트, 룰렛(적합도 비례), 랭킹(순위별 면적) \/\n교배 : 일점, 다점, 균등, 산술교차 \/ 돌연변이 : 정적, 동적"},{"토픽":"과적합\n(오버피팅)","키워드":"하 베 로 비\n부 전 복 피\n추 다 정 차 앙 조 드 교","도식":null,"정의":"훈련 데이터에만 과하게 학습,train에서는 높은 정확도지만테스트\/실제 시 성능 저하 문제","내용":"High Variance, Low Bias\n원인 : Data 부족, 전처리 잘못, 모델 복잡, Feature 과도\nData : Data 추가, Data 다양화(편향 제거)\n모델 : 정규화, 차원 축소(Feature 줄임), 앙상블\n학습 : 조기 종료(정확도 변화 없을 시), 드롭아웃(은닉 노드)\n검증 : 교차 검증(3단락 \/ 얜 Test Data 과적합 방지)"},{"토픽":"교차검증\n(Cross Validation)","키워드":"홀 케 엘 피 랜","도식":null,"정의":"Data의 모든 부분을 사용해모델을 검증함으로써Test Set에 과적합되는 것을방지하는 검증 기법","내용":"Hold Out : train, validation, test \/ valid로 검증 후 최종 test\n\nK Fold : K개의 Fold, K-1개 Train, 1개 Test, Test 수 K번\n\nLOOCV : n-1개로 Train, 1개로 Test, Test 수 n번\n\nLPOCV : 가장 연산 많음, Test 횟수 nCp\n\nRLT : 랜덤 비복원 추출"},{"토픽":"교차검증 기법\n(Cross Validation)","키워드":"홀 케 랜 엘 피","도식":null,"정의":null,"내용":"비소모적 교차 검증(Non-Exhaustive cross-validation)\n홀드 아웃 : 비복원\nK-Fold : 동등분할\n랜덤서브샘플링 : 홀드아웃 반복\nRLT : 비복원 추출, 랜덤추출, 평균 오류율 계산\n\n소모적 교차 검증(Exhaustive cross-validation)\nLOOCV : 데이터 한개만 선택\nLPOCV : 전체데이터 N개 중 여러개"},{"토픽":"언더피팅\n(Underfitting)","키워드":"로 베 하 비\n언 데 간 피 학\n피 분\n에 복","도식":null,"정의":"모델이 너무 간단하여Train set도 다 학습하지 못해학습 오류가 줄지 않는 현상","내용":"Low Variance High Bias\n원인 : Data 부족, 모델 너무 간단(Feature 너무 조금 반영),\n학습 반복 부족\n해결 : 피쳐 추가, 분산 높은 모델(의사결정나무, KNN, SVM)\n오버\/언더피팅 그래프 : Y축 에러 \/ X축 모델 복잡도"},{"토픽":"과적합, 언더피팅균형점","키워드":null,"도식":null,"정의":null,"내용":"모델의 Feature 을 조절 > Bias와 Variance 를 측정 > 에러율이 최저 지점 탐색\nTrain Data 와 Test Data 대상으로 각각 에러율을 측정해 Test 에러가 Train 에러보다 높을 시 오버피팅\nTest 와 Train 에러가 비슷하게 높을 시 언더피팅 해결 도모\n"},{"토픽":"training-serving skew","키워드":"스 배","도식":null,"정의":"머신러닝 모델의training 시 성능과 serving 시 성능 간의 차이","내용":"스키마 편향 : train이랑 serve Data가 동일 스키마 준수 X\n(제공 Data에 새 특성 추가, train 땐 정수였다 제공 땐 float)\n배포 편향 : 학습 Data 특성값 분포가 serve의 분포랑 달라\n해결 : 시스템 or Data의 변화로 인한 격차 안생기게 모니터링"},{"토픽":"Short Fat 구조","키워드":"회 로 베 하 바 언","도식":null,"정의":"Data set에서입력변수가 많고Y값은 다 비슷하게 나오는,상대적으로 Data가 적은 구조","내용":"이상적 Data는 Thin & Tail 해서 Y 다양해야 되나 얜 Y 비슷\n> 과적합, 다중공선성 발생 > 릿지, 라쏘, PCA로 해결\n회귀 : Low Variance High Bias, 언더피팅(학습\/test 다↓)\n인공신경망 : High Variance Low Bias, 과적합"},{"토픽":"편향성","키워드":"고 인 샘 히 롱","도식":null,"정의":"데이터가 전체를 표현 못하고일부만을 표현하는 현상","내용":"수집 : 고의 \/ 인간 \/ 샘플 \/ Hidden \/ Long tail(특정 빠짐)\n가공 : 정제\/변환 시 편향\n학습 : 잘못된 학습(의사가 보통 남자 > 여자는 떨어뜨림)"},{"토픽":"필터 버블\n(에코 챔버)","키워드":null,"도식":null,"정의":"원래 가진 생각\/편견 등 확인하려는 경향성으로 확증 편향, 정보여과 현상의 확산","내용":"유튜브 개인 맞춤 알고리즘 등 -> 소비자 선택권 제한\n대응 : Data 충분 학습, XAI, GDPR,\n미국 '필터버블 투명성 법안' 발의"},{"토픽":"모델 드리프트\n(Drift)","키워드":"컨 데 업 프","도식":null,"정의":"현실 세계의 환경 변화에 따라모델의 성능이 저하되는 현상","내용":"컨셉 Drift : 예측 변수의 의미 변경('금융사기' 정의가 바뀜)\nData Drift : 통계 속성이 변함(계절\/상품 변경, 선호도 변화)\nUpstream : Data 파이프라인 변화(더 이상 변수 측정 안함)\nPrediction : Data 분포가 변해 Input-예측치 관계 변화"},{"토픽":"모델 드리프트 해결 방안","키워드":"정 감 판 해\n최 모 씨","도식":null,"정의":null,"내용":"Drift Signal 정의 > Signal 감지 > Drift 판정 > Drift 해결\n- Signal : 정확도, 분포, 관측값과 평균 사이 관계 변화 등\n- 판정 : 이상치 여부, 신호의 정상성 확인(패턴 있으면 무시)\n- 해결 : CI\/CD, 최근 Data에 가중치 부여, 모델 수정"},{"토픽":"빅데이터 통합 아키텍처","키워드":"수 저처 정분 서","도식":null,"정의":"플랫폼, 분석도구, 분석기법","내용":"수집, 저장\/처리, 정제\/분석, 서비스\nFTP, OpenAPI, RSS, 변환\/정제\/통합, HDFS,\nHbase(NoSQL), 보안, 기술통계\/상관\/회귀\/분산,\n마이닝(분예군연), SNA, 시각화"},{"토픽":"빅데이터품질관리 체계","키워드":"원 조 프\n정일 유보적 접근 완유준\n대 진 결 개 통","도식":null,"정의":null,"내용":"원칙, 조직, 프로세스\n지표 : 정확, 일관, 유용, 보안, 적시, 접근 + 완전 유용 준비\n진단 대상 > 진단 수행 > 결과 > 개선 > 품질 통제"},{"토픽":"빅데이터 플랫폼 아키텍처","키워드":"5V : 볼 속 다 정 가\n범 수 정 비 반","도식":null,"정의":"빅데이터 기술을 잘 사용할 수 있도록 준비된 환경으로, 빅데이터 세부 기술 집합체","내용":"5V : 크기(Volume), 속도(Velocity), 다양성(Variety), 정확성(Veracity), 가치(Value)\n\n구성요소\n구성 인프라 : 노드 \/ 스토리지 \/ 네트워크\n구성 모듈 : 자원배치 \/ 노드관리 \/ 데이터관리 \/ 자원관리\/ 서비스관리 \/ 모니터링 \/ 보안\n\n데이터구조\n범주형 : 명목형, 순서형\n수치형 : 이산형, 연속형\n데이터 유형 : 정형, 비정형, 반정형"},{"토픽":"빅데이터분석도구","키워드":"원 관 분 편\n기 사 벤 품\n도 파 스 알 썬","도식":null,"정의":"마이닝, 인공지능, 자연어처리 등 기반, Data를 빠르게 분석해 문제해결\/의사결정 하는 도구","내용":"원칙 : 관리(도구, 언어) \/ 분석 \/ 편의\n기준 : 사용성, 벤더(유지\/개선력), 품질(기능성, 성능, 보안)\n도구 : 파워 쿼리\/피벗(엑셀제공), SPSS, R, 파이썬,\n구글 트랜드"},{"토픽":"ISO 20547","키워드":"프 유 참 보 표","도식":null,"정의":"빅데이터 시스템 간 상호호환, 원활한 Data 유통을 위한빅데이터 참조 아키텍처 표준","내용":"구성요소(Part1 ~ 5)\nPart1 : 프레임워크 및 응용 프로세스\nPart2 : 유즈케이스 및 기술적 고려사항\nPart3 : 참조 아키텍처\nPart4 : 보안 및 개인정보보호\nPart5 : 표준 로드맵"},{"토픽":"하둡에코시스템","키워드":"얀 맵 피 하 임 마\n주 압 우 카","도식":null,"정의":"데이터 수집\/저장\/분석\/관리 등빅데이터 분석을 위한다양한 오픈소스 기반 시스템","내용":"수집 : 척와, 플룸, Scribe(센스플스척) \/ Sqoop, Hiho\n저장 : HDFS, Hbase, Tajo, RDB, NoSQL\n처리\/분석 : 얀, 맵리듀스, PIG, Hive, Impala, Mahout\n관리\/지원 : 주키퍼, Avro, Oozie(워크플로우), Hcatalog"},{"토픽":"하둡","키워드":"6V 볼 속 다 정 가 시","도식":null,"정의":"Data 분산 저장\/처리하는HDFS, 맵리듀스 등자바 기반 오픈소스 프레임워크","내용":"특징 : Volume(페타바이트) \/ 다양성(정형,반정형,비정형) \/ 속도(실시간데이터) \/ 정확성(의사결정 지원으로 신뢰성 제고) \/ 시각화(사용자친화적) \/ 가치(비즈니스, 공공정책, 방향성 제시 등 실질적 가치 제공)\n\n맵리듀스 : 대용량 Data를 분산 병렬 처리\/생성하는 SW\nHDFS : 대용량 Data를 분산 서버에 저장\/신속 처리 File Sys\nData 일괄 처리, Disk 기반"},{"토픽":"람다 아키텍처","키워드":"배 스 서 쿼","도식":null,"정의":null,"내용":"Data > Batch \/ Speed Layer > Serving Layer > 쿼리\n- Batch : 마스터 Data Set, 선계산 정보\n- Speed : 스트림 프로세스 > 증분 정보\n- Serving : 배치 뷰, 실시간 뷰를 Merge"},{"토픽":"카파 아키텍처","키워드":"스 서 쿼","도식":null,"정의":null,"내용":"Data > Speed Layer > Serving Layer > 쿼리\n- Speed : 작업 버전 N-1, N, N+1\n- Serving : 결과 테이블 N-1, N, N+1"},{"토픽":"제타 아키텍처","키워드":"서 웹 광 하 디 실 엔","도식":null,"정의":"실시간 Data 스토리지,분산 파일 시스템 등 기반으로비즈니스에 데이터 통합하고Data를 효율적 배포하는 구조","내용":"람다, 카파 이후\n서버 > 하둡 클러스터 > 실행 엔진\n- 서버 : 웹 서버(컨테이너) ↔ 광고 엔진(컨테이너)\n- 하둡 클러스터 : DFS(분산 파일), 실시간 Data 스토리지"},{"토픽":"스파크","키워드":"하 얀 메 스 코 분\n일 파 테 인 반","도식":null,"정의":"Data 일괄처리 시디스크 I\/O 효율화,인메모리 기반 분산처리 시스템","내용":"HDFS \/ 얀, Mesos, Standalone \/ 스파크 코어 \/ 분석\n- 분석 : SQL, 스트리밍, 그래프, 머신러닝\n일괄 처리, 파일\/테이블 단위, 인메모리, 반복에 강함"},{"토픽":"스톰","키워드":"주 마 님 워 슈 네 제\n스 튜 인 빠 다","도식":null,"정의":"데이터의 실시간 처리 위해범용 분산 환경 기반실시간 데이터 처리 시스템","내용":"주키퍼(코디네이터) ↔ 마스터(님부스), 워커(슈퍼바이저)\n- 워커 노드 : 슈퍼바이저(워커 모니터링) ↔ 워커 n개\n- 마스터와 워커 사이는 Netty\/ZeroMQ로 연결\n스트리밍, 튜플 단위, 인메모리, 속도 빠름, 다양 질의에 굿"},{"토픽":"거대 AI","키워드":null,"도식":null,"정의":"초대규모 Data와빠른 컴퓨팅 자원으로전이학습 등 알고리즘 활용한고성능 AI","내용":"초대규모 Data : 5천억 단어\n컴퓨팅 자원 : 슈퍼컴, 분산학습, 연합학습\n알고리즘, 모델 : 전이학습(파인튜닝), 복합 인공지능, NLP\n네이버 하이퍼 클로바 \/ 카카오 빅모델 등"},{"토픽":"초거대 AI","키워드":"A D I O S","도식":null,"정의":"몇천억 이상의 파라미터를 갖고언어, 영상, 이미지를 이해,데이터 추론\/생성까지수행하는 창의적 AI\n\n심층 신경망으로 구현된 크기가 매우 큰 AI로 인공신경망의 파라미터(매개변수)가 무수히 많은 인공지능 모델","내용":"발전 : 통계기반 > 트랜스포머 알고리즘 > Bert, GPT > GPT3 > ChatGPT, GPT4, Bard\n\n공공분야 초거대 AI 구축방안 : A.D.I.O.S\nAI 모델 - AI모델선정 \/ 사전학습모델 \/ 도메인별 조정 \/ SFT(Supervised Fine Tuning)학습 \/ RLHF\nData : 원천데이터 수집 전략 \/ 학습데이터 제작방안 \/ 데이터유형 \/ 데이터 정제\nInfra : 인프라구축, 자원확보방안\nOperation : 학습 \/ 추론 및 배포 \/ 추가학습\nService : 기본서비스 \/ 분야별 서비스"},{"토픽":"파운데이션 모델","키워드":"방 자 대 제 트 G 성 창 균","도식":null,"정의":"LLM을 사용자가 원하는 목적에 학습 시키기 위해 방대한 양의 데이터를 사전에 학습시킨 반제품 형태의 AI 기초 모델","내용":"특징\n방대한 데이터, 자기지도, 대형모델, 제로샷, GPU클러스터링\n창발성, 균일화, 트랜스포머 아키텍처\n\n제반기술 : 대용량 데이터 구축, 자기지도학습, 트랜스포머아키텍처, 컴퓨팅 성능 향상"},{"토픽":"XAI","키워드":null,"도식":null,"정의":"AI 모델의 의사결정 근거를사용자 레벨에서 설명하는AI기술(모델)","내용":"Data > 학습 > 설명가능 모델\/설명 I\/F ↔ 사용자\nAI 블랙박스 해결"},{"토픽":"XAI 구현기법","키워드":null,"도식":null,"정의":null,"내용":"모델 비종속적 : LIME, BRL, SHAP\n모델 종속적 : LRP, DGNN 생성경계 고려, 탐색적 샘플링 방식, Rule Extraction, Zero shot learning"},{"토픽":"AI 반도체","키워드":null,"도식":null,"정의":null,"내용":"HW : 프로세서, 메모리, 디스크, 인터커넥터\nSW : 운영체제, 컴파일러, 라이브러리, 드라이버\n- AI 최적화를 돕는 SW"},{"토픽":"뉴로모픽 컴퓨팅","키워드":"씨 멤 통","도식":null,"정의":"폰노이만 방식 탈피하여CPU, MEM, 통신 병렬 처리,뇌의 action potential 모방한다수의 저전력 코어 컴퓨팅","내용":"폰 노이만 : CPU ← bus > Mem(1회 1입출력)\n뉴로모픽 : CPU + Mem + 통신 융합(병렬 입출력)\n한 코어에서 발생한 전기신호는 다음 코어로 빨리 전달,\n둘 사이 연관 높으면 저항을 낮추어 전류량을 자동 증가"},{"토픽":"뉴로모픽반도체","키워드":"코 입 출 시크\n신 웨 피 스","도식":null,"정의":"뉴로모픽 컴퓨팅 위한다수의 저전력 코어 컴퓨팅반도체","내용":"synaptic core : 입력 뉴런(이전 코어에서 신호 수신), 출력\n뉴런(다음 코어로 신호 전달), 시냅틱 crossbar(입출력 연결)\n처리신호 : weight(출력>입력 신호 전달 활성화), PRNG\n(뉴런에 대한 난수 W), spike(뉴런 통해 전달되는 임계 전압)\n코어는 학습으로 크로스바 활성\/비활성, spike 전달여부 결정"},{"토픽":"SNN\n(Spiking Neural Network)","키워드":"입 시 뉴 출\nDNN 개선","도식":null,"정의":"뉴로모픽 핵심 기술,두뇌의 정보 전달 과정을 모사, 대량 Data를 적은 스파이크로처리하는 인공 신경망","내용":"원칙 : 고성능, 저전력, On-Chip Learning\n모델(LIF) : 입력Spike > 시냅스(w) > 뉴런(Σ) > 출력Spike\n* 뉴런에서 결합(Σ)이 임계 넘어야 다음 뉴런으로 spike 출력\n* LIF : Leaky Integrated-and-Fire"},{"토픽":"SNN 학습방법","키워드":"DNN 개선\n뉴로모픽칩이 HW라면\nSNN은 SW측면 뉴로모픽","도식":null,"정의":"STDP(Spike-Timing-DependentPlasticity)","내용":"입\/출 뉴런의 spike 발생시간 상 상관관계로 시냅스 W 조절,\nΔt (시냅스 전\/후 뉴런 사이의 스파이크 발생 시간차이)\n작을수록 두 뉴런은 밀접한 관계"},{"토픽":"MLPerf","키워드":"비 추 언 메","도식":null,"정의":"다양한 AI 반도체가얼마나 빨리정확한 결과값을 출력하는지평가하는 벤치마킹 테스트","내용":"적용된 SW, Framework, Library 등이 다양해 표준지표 필요\n컴퓨터 비전 : 이미지 분류, 물체 검출(경량), 물체 검출(중량)\n추천 시스템 : 추천, 강화학습\n언어 처리 : 자연어 처리(NLP), 자동 음성인식(ASR)\n메디컬 이미지 : 생체 의학 이미지 분할"},{"토픽":"적대적 공격","키워드":"회 전 중 추","도식":null,"정의":"머신러닝 모델의 신뢰 감소,Data 탈취 등 위해교묘하게 AI를 속이는알고리즘 취약점 악용 공격","내용":"Evasion(회피) : Test 시, Noise 주입, Data 교란 \/ Data 공격\nInversion(전도) : 수집 시, 질의 던져 Data 유추 \/ Data 공격\nPoisoning(중독) : 훈련 시, 악의적 Data 주입 \/ 모델 공격\nExtraction(추출) : 역공학, 쿼리로 모델 탈취 \/ 모델 공격"},{"토픽":"적대적 공격 방어","키워드":"훈 노 암 난 횟 탐","도식":null,"정의":null,"내용":"적대적 훈련 : 모든 적대적 사례 학습으로 Poisoning 해결\n결과값 노출 방지 \/ 암호화 \/ 난독화 \/ 쿼리 횟수 제한\n공격 탐지 : 비교 모델 추가, 원 모델과 결과 비교, 차이 탐지\n- 대조군과 비교해서 Noise 추가 버전 걸러냄"},{"토픽":"인공지능 역기능","키워드":null,"도식":null,"정의":null,"내용":"윤리 : 편향(필터 버블, 에코 챔버), 차별, 존엄성 저해, 공격\n사회 : 일자리 감소, 디지털 격차, 디지털 소외\n기술 : AI 판단 오류, Data 노출, Privacy 이슈\n경제 : 저작권 이슈(ChatGPT의 작품), 피해보상 주체 이슈"},{"토픽":"인공지능개발\/활용에 관한인권 가이드라인","키워드":"존 결 투 차 영 위","도식":null,"정의":null,"내용":"개인 : 인간 존엄성(헌법 제10조), 자기 결정권(정보 통제권)\n사회 : 투명성과 설명 의무, 차별 금지 노력\n정부 : 인공지능 인권 영향평가, 위험도 등급 및 법\/제도 마련"},{"토픽":"인공지능위험도 등급","키워드":"금 고 제 낮","도식":null,"정의":"인공지능의인권\/안전 등 영향 분석해AI 사용 수준을 결정하게 하는위험도 수준","내용":"AI 금지 영역 : 인권\/안전 고위험\nAI 고위험 영역 : 제한적으로 사용해야 함(AI 채용 등)\n제한적 위험 : 일부 기능 제한\n위험성 낮음 : 텍스트 분류, 영상 분석 등"},{"토픽":"신뢰가능 AI원칙\n(OECD)","키워드":"포지 인공 투설 보안 책","도식":null,"정의":"알고리즘\/Data 편향, privacy침해 등 대응 위한 AI 원칙","내용":"포용 성장, 지속가능발전 \/ 인간중심, 공정성 \/ 투명성, 설명가능성 \/ 보안, 안전성 \/ 책임성"},{"토픽":"신뢰가능 AI거버넌스프레임워크","키워드":"거 의 책 커","도식":null,"정의":"신뢰가능 AI 구현을 위한체계적 프레임워크","내용":"거버넌스 구축, 운영 \/ 의사결정 모델 \/ 책임 있는 Data 관리 \/ 고객 커뮤니케이션"},{"토픽":"인공지능 윤리기준3대 기본원칙","키워드":"합 공 존","도식":null,"정의":"과기부와 정보통신정책연구원제정하고 4차산업혁명위원회가심의\/의결한,인공지능 개발\/활용 윤리 기준","내용":"기술의 합목적성 : 사용자의 목적\/의도에 따른 개발\/활용,\n궁극적으로 인류의 삶과 번영 위해 개발 및 활용되어야 함\n사회의 공공선 : 가능한 많은 사람의 안녕\/행복\/공익 추구,\n인류의 보편적 복지 향상, 사회적 약자와 취약층 접근성 보장\n인간 존엄성 : 인간은 AI와 교환 불가한 가치가 있음\n+ AI는 인간 생명\/정신\/신체에 피해 없는 선에서 개발\/활용"},{"토픽":"인공지능 윤리기준 10대 핵심요건","키워드":"연 인 데 투 보 다 책 안 침 공","도식":null,"정의":"존엄성 : 인권보장 \/ 프라이버시보호 \/ 다양성존중\n공공선 : 침해금지 \/ 공공성 \/ 연대성\n합목적성 : 데이터 관리 \/ 책임성 \/ 안전성 \/ 투명성","내용":"인권 보장, 프라이버시 보호, 다양성 존중, 침해금지, 공공선,\n연대성, 데이터 관리, 책임성, 안전성, 투명성\n- 침해금지 : 인간에 해를 입히는 목적 X, AI 잠재 위험 대응\n- 공공선 : 개인 행복 뿐 아니라 사회 공동 이익 위해 활용\n- 연대성 : 다양한 주체들의 공정한 참여 기회, 미래세대 배려\n- 데이터 관리 : 편향성 최소화, 품질 관리, 목적에 맞게 사용\n- 책임성 : 개발\/활용 책임주체 설정, 설계\/개발자 책임소재"},{"토픽":"인공지능 윤리기준 KS표준 윤리항목","키워드":"무 공 책 자 연 투 지 사 신 편","도식":null,"정의":null,"내용":"무해성 \/ 공정성 \/ 책임성 \/ 자율성 \/ 연대성 \/ 투명성 \/ 지속성 \/ 사생활 보장성 \/ 신뢰성 \/ 편의성"},{"토픽":"챗봇 윤리원칙","키워드":"존권 프정 다 투 책","도식":null,"정의":"챗봇 서비스의 신뢰도를 높이고 윤리적 문제에 대응하기 위해 한국인터넷자율정책기구에서 제정한 5가지 원칙 가이드 라인","내용":"- 인간의 존엄성 및 권리 존중 원칙\n인격, 존엄성 존중 \/ 관련 법령 준수 \/ 권리, 윤리적 가치 존중\n- 프라이버시 보호 및 정보보안 원칙\n개인정보 보호 \/ 개인정보 오 남용, 부당한 공유방지\n- 다양성 존중 원칙\n동등한 접근성 \/ 편향성 방지 \/ 부당처벌금지 \/ 아동,청소년,약자보호\n- 투명성 원칙\n챗봇임을 공지 \/ 용도 및 특성 쉽게 설명\n- 책임 원칙\n사회적 영향고려 \/ 책무성 체계 \/ 상생"},{"토픽":"AI 위험관리 프레임워크\n(AI RMF)","키워드":"생 사 조\n유 안 보 복 책 투 해 설 공 개\n거 위 관 측","도식":null,"정의":"AI 시스템의 잠재적인 부정적 영향을 최소화하고긍정적인 영향 극대화를 목표로 NIST에서 제정한 프레임워크","내용":"AI시스템 잠재적 피해 : 사람에 대한 피해, 조직에 대한 피해, 생태계에 대한 피해\n신뢰할 수 있는 AI 시스템 특징 : 유효성, 안전성, 보안 및 복원성, 책임과 투명성, 설명 및 해석 가능성, 개인정보보호, 공정성\nAI리스크 관리, 신뢰있는 시스템 구축 위한 네가지 핵심 기능 : 거버넌스, 맵핑(위험식별), 측정, 관리"},{"토픽":"AGI\n(범용AI)","키워드":"자 멀 창\n\n노 신 유 전 거 슈","도식":null,"정의":"특화된 분야가 아닌 사람처럼 다양한 분야에서 능력을 발휘하는 인공지능 ","내용":"자율성장 \/ 멀티모달리티 \/ 창작(창발성)\n\n구글 AGI 분류 체계\nLevel 0 : No AI(AI 미진입) - 단순 연산\nLevel 1 : Emerging(신진) - 비숙련자 성인\nLevel 2 : Competent(유능함) - 상위 50%\nLevel 3 : Expert(전문가) - 상위 10%\nLevel 4 : Virtuoso(거장) - 상위 1%\nLevel 5 : Superhuman(슈퍼휴먼) - 초월"},{"토픽":"LLM 벤치마크","키워드":"앍 엠 헬 트","도식":null,"정의":"LLM의 독해, 이해, 분류 등 언어 능력을 평가하기 위한 기준이 되는 Data Set\n- MMLU(Massive Multitask Language Understanding) 등 4가지 벤치마크 사용","내용":"① ARC(AI2 Reasoning Challenge) : 초등학교 수준의 과학문제\n\n② MMLU : 초등수학, 미국역사, 컴퓨터 과학, 도덕적 분쟁, 법률 등 57개 주제\n\n③ HellaSwag : 상식 추론능력 평가를 위한 데이터 셋\n\n④ TruthfulQA(진실성 검사) : 할루시네이션 정도를 평가하는 데이터 셋"}]